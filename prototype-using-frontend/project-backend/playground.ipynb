{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24211645",
   "metadata": {},
   "source": [
    "# ğŸ§ª Mock LeetCode Assistant Server (Testing Version)\n",
    "\n",
    "A **hardcoded testing server** that simulates LeetCode problem analysis without connecting to OpenAI's API. Returns a pre-written Two Sum solution for development and testing purposes.\n",
    "\n",
    "## ğŸ“‹ Endpoint Details\n",
    "\n",
    "**`POST /process-multiple-frames-stream`**\n",
    "\n",
    "**Input:** \n",
    "- Form data with image frames (`frame_0`, `frame_1`, etc.)\n",
    "- Frame count metadata\n",
    "\n",
    "**Output:** \n",
    "- Server-Sent Events stream with mock Two Sum solution\n",
    "- Frames saved to local `frames/` directory\n",
    "- Mock detected text and problem explanation\n",
    "\n",
    "## ğŸš€ Usage\n",
    "\n",
    "Run the server and send POST request with image frames to get streaming mock AI response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "173fb8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [21908]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Starting FRAME READER & SAVER server on http://localhost:8000\n",
      "ğŸ“ Frames will be saved to: frames/ directory\n",
      "ğŸ“¡ Now includes streaming Two Sum solution!\n",
      "ğŸ’¡ Send your frontend request to save frames and get streaming output!\n",
      "==================================================\n",
      "ğŸ”„ READING AND SAVING FRAMES...\n",
      "ğŸ“‹ Content-Type: multipart/form-data; boundary=----WebKitFormBoundary4kuMiTruikFaqJ7b\n",
      "==================================================\n",
      "ğŸ”„ READING AND SAVING FRAMES...\n",
      "ğŸ“‹ Content-Type: multipart/form-data; boundary=----WebKitFormBoundaryUSNpc3M7s1v046aq\n",
      "ğŸ“¥ Form data items: 5\n",
      "ğŸ” Form keys: ['frame_0', 'frame_1', 'frame_2', 'frame_3', 'frame_count']\n",
      "ğŸ“¥ Form data items: 5\n",
      "ğŸ” Form keys: ['frame_0', 'frame_1', 'frame_2', 'frame_3', 'frame_count']\n",
      "ğŸ“ Processing frame_0: 1180743 bytes\n",
      "âœ… Saved frame_0: frames/frame_20250709_122518_frame_0.png (1280x720)\n",
      "ğŸ“ Processing frame_0: 1180743 bytes\n",
      "âœ… Saved frame_0: frames/frame_20250709_122518_frame_0.png (1280x720)\n",
      "ğŸ“ Processing frame_1: 1166641 bytes\n",
      "âœ… Saved frame_1: frames/frame_20250709_122518_frame_1.png (1280x720)\n",
      "ğŸ“ Processing frame_2: 1182030 bytes\n",
      "âœ… Saved frame_2: frames/frame_20250709_122518_frame_2.png (1280x720)\n",
      "ğŸ“ Processing frame_1: 1166641 bytes\n",
      "âœ… Saved frame_1: frames/frame_20250709_122518_frame_1.png (1280x720)\n",
      "ğŸ“ Processing frame_2: 1182030 bytes\n",
      "âœ… Saved frame_2: frames/frame_20250709_122518_frame_2.png (1280x720)\n",
      "ğŸ“ Processing frame_3: 1178373 bytes\n",
      "âœ… Saved frame_3: frames/frame_20250709_122518_frame_3.png (1280x720)\n",
      "ğŸ“Š Expected frame count: 4\n",
      "ğŸ‰ Successfully saved 4 frames to frames/ folder!\n",
      "INFO:     127.0.0.1:57640 - \"POST /process-multiple-frames-stream HTTP/1.1\" 200 OK\n",
      "ğŸ“ Processing frame_3: 1178373 bytes\n",
      "âœ… Saved frame_3: frames/frame_20250709_122518_frame_3.png (1280x720)\n",
      "ğŸ“Š Expected frame count: 4\n",
      "ğŸ‰ Successfully saved 4 frames to frames/ folder!\n",
      "INFO:     127.0.0.1:57641 - \"POST /process-multiple-frames-stream HTTP/1.1\" 200 OK\n",
      "==================================================\n",
      "ğŸ”„ READING AND SAVING FRAMES...\n",
      "ğŸ“‹ Content-Type: multipart/form-data; boundary=----WebKitFormBoundaryqki1UfYCWQLMb4pT\n",
      "ğŸ“¥ Form data items: 5\n",
      "ğŸ” Form keys: ['frame_0', 'frame_1', 'frame_2', 'frame_3', 'frame_count']\n",
      "ğŸ“ Processing frame_0: 1174859 bytes\n",
      "âœ… Saved frame_0: frames/frame_20250709_122614_frame_0.png (1280x720)\n",
      "ğŸ“ Processing frame_1: 1175226 bytes\n",
      "âœ… Saved frame_1: frames/frame_20250709_122614_frame_1.png (1280x720)\n",
      "==================================================\n",
      "ğŸ”„ READING AND SAVING FRAMES...\n",
      "ğŸ“‹ Content-Type: multipart/form-data; boundary=----WebKitFormBoundary6BW8XpnPBhuGqh6J\n",
      "ğŸ“ Processing frame_2: 1171640 bytes\n",
      "âœ… Saved frame_2: frames/frame_20250709_122614_frame_2.png (1280x720)\n",
      "ğŸ“ Processing frame_3: 1174039 bytes\n",
      "âœ… Saved frame_3: frames/frame_20250709_122614_frame_3.png (1280x720)\n",
      "ğŸ“Š Expected frame count: 4\n",
      "ğŸ‰ Successfully saved 4 frames to frames/ folder!\n",
      "INFO:     127.0.0.1:57642 - \"POST /process-multiple-frames-stream HTTP/1.1\" 200 OK\n",
      "ğŸ“¥ Form data items: 5\n",
      "ğŸ” Form keys: ['frame_0', 'frame_1', 'frame_2', 'frame_3', 'frame_count']\n",
      "ğŸ“ Processing frame_0: 1174859 bytes\n",
      "âœ… Saved frame_0: frames/frame_20250709_122615_frame_0.png (1280x720)\n",
      "ğŸ“ Processing frame_1: 1175226 bytes\n",
      "âœ… Saved frame_1: frames/frame_20250709_122615_frame_1.png (1280x720)\n",
      "ğŸ“ Processing frame_2: 1171640 bytes\n",
      "âœ… Saved frame_2: frames/frame_20250709_122615_frame_2.png (1280x720)\n",
      "ğŸ“ Processing frame_3: 1174039 bytes\n",
      "âœ… Saved frame_3: frames/frame_20250709_122615_frame_3.png (1280x720)\n",
      "ğŸ“Š Expected frame count: 4\n",
      "ğŸ‰ Successfully saved 4 frames to frames/ folder!\n",
      "INFO:     127.0.0.1:57685 - \"POST /process-multiple-frames-stream HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Shutting down\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [21908]\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from fastapi import FastAPI, Request\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi.responses import JSONResponse, StreamingResponse\n",
    "from PIL import Image\n",
    "import json\n",
    "import asyncio\n",
    "import io\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Enable CORS\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "@app.post(\"/process-multiple-frames-stream\")\n",
    "async def simple_frame_reader_with_save(request: Request):\n",
    "    \"\"\"Simple endpoint that reads frames AND saves them to frames folder, then streams text\"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(\"=\" * 50)\n",
    "        print(\"ğŸ”„ READING AND SAVING FRAMES...\")\n",
    "        \n",
    "        # Get content type\n",
    "        content_type = request.headers.get(\"content-type\", \"\")\n",
    "        print(f\"ğŸ“‹ Content-Type: {content_type}\")\n",
    "        \n",
    "        # Parse form data\n",
    "        form_data = await request.form()\n",
    "        print(f\"ğŸ“¥ Form data items: {len(form_data)}\")\n",
    "        print(f\"ğŸ” Form keys: {list(form_data.keys())}\")\n",
    "        \n",
    "        # Create frames directory\n",
    "        os.makedirs(\"frames\", exist_ok=True)\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        # Process and save each frame\n",
    "        saved_frames = []\n",
    "        frame_count = 0\n",
    "        \n",
    "        for key, value in form_data.items():\n",
    "            if key.startswith('frame_') and hasattr(value, 'read'):\n",
    "                try:\n",
    "                    # Read file data\n",
    "                    file_data = await value.read()\n",
    "                    print(f\"ğŸ“ Processing {key}: {len(file_data)} bytes\")\n",
    "                    \n",
    "                    # Open image\n",
    "                    img = Image.open(io.BytesIO(file_data))\n",
    "                    \n",
    "                    # Save frame with timestamp\n",
    "                    save_path = f\"frames/frame_{timestamp}_{key}.png\"\n",
    "                    img.save(save_path)\n",
    "                    \n",
    "                    saved_frames.append({\n",
    "                        \"key\": key,\n",
    "                        \"filename\": getattr(value, 'filename', 'unknown'),\n",
    "                        \"size_bytes\": len(file_data),\n",
    "                        \"image_size\": f\"{img.width}x{img.height}\",\n",
    "                        \"saved_path\": save_path\n",
    "                    })\n",
    "                    \n",
    "                    frame_count += 1\n",
    "                    print(f\"âœ… Saved {key}: {save_path} ({img.width}x{img.height})\")\n",
    "                    \n",
    "                except Exception as frame_error:\n",
    "                    print(f\"âŒ Error processing {key}: {frame_error}\")\n",
    "            \n",
    "            elif key == 'frame_count':\n",
    "                expected_count = str(value)\n",
    "                print(f\"ğŸ“Š Expected frame count: {expected_count}\")\n",
    "        \n",
    "        print(f\"ğŸ‰ Successfully saved {frame_count} frames to frames/ folder!\")\n",
    "        \n",
    "        # Now create streaming response with initial success data + streaming text\n",
    "        async def generate_stream():\n",
    "            # First yield the success response\n",
    "            initial_response = {\n",
    "                \"success\": True,\n",
    "                \"message\": f\"Successfully received and saved {frame_count} frames!\",\n",
    "                \"frames_saved\": saved_frames,\n",
    "                \"timestamp\": timestamp,\n",
    "                \"save_directory\": \"frames/\",\n",
    "                \"streaming\": True,\n",
    "                \"frame_count\": frame_count,\n",
    "                \"type\": \"initial\"\n",
    "            }\n",
    "            yield f\"data: {json.dumps(initial_response)}\\n\\n\"\n",
    "            \n",
    "            # Small delay before starting analysis\n",
    "            await asyncio.sleep(0.3)\n",
    "            \n",
    "            # Then stream mock GPT-like Two Sum solution\n",
    "            mock_gpt_responses = [\n",
    "                \"ğŸ¤– I can see you're working on the Two Sum problem! Let me help you solve this step by step.\\n\",\n",
    "                \"ğŸ“‹ **Problem Analysis:**\\nGiven an array of integers `nums` and an integer `target`, return indices of two numbers that add up to `target`.\\n\",\n",
    "                \"ğŸ’¡ **Approach 1: Brute Force**\\nWe could use nested loops to check every pair, but that would be O(nÂ²) time complexity.\\n\",\n",
    "                \"âš¡ **Better Approach: Hash Map**\\nLet's use a hash map to solve this in O(n) time!\\n\",\n",
    "                \"```python\\ndef twoSum(nums, target):\\n    num_map = {}\\n    for i, num in enumerate(nums):\\n        complement = target - num\\n        if complement in num_map:\\n            return [num_map[complement], i]\\n        num_map[num] = i\\n```\\n\",\n",
    "                \"ğŸ” **How it works:**\\n1. Create an empty hash map\\n2. For each number, calculate its complement (target - current number)\\n3. Check if complement exists in hash map\\n4. If yes, return the indices; if no, store current number and index\\n\",\n",
    "                \"ğŸ“Š **Time Complexity:** O(n) - single pass through array\\n**Space Complexity:** O(n) - hash map storage\\n\",\n",
    "                \"ğŸ§ª **Test with your example:**\\n`nums = [2,7,11,15], target = 9`\\n- i=0, num=2, complement=7, not in map, store {2:0}\\n- i=1, num=7, complement=2, found at index 0, return [0,1]\\n\",\n",
    "                \"âœ¨ **Alternative One-liner (Python):**\\n```python\\ndef twoSum(nums, target):\\n    seen = {}\\n    return next(([seen[target-n], i] for i, n in enumerate(nums) if target-n in seen or seen.setdefault(n, i)), None)\\n```\\n\",\n",
    "                \"ğŸ¯ **Key Insights:**\\n- Hash maps provide O(1) average lookup time\\n- We only need one pass through the array\\n- Always check if complement exists before storing current number\\n\",\n",
    "                \"ğŸ† **Solution Complete!** This approach efficiently solves Two Sum in linear time.\\n\"\n",
    "            ]\n",
    "            \n",
    "            for i, response in enumerate(mock_gpt_responses):\n",
    "                await asyncio.sleep(0.8)  # Slightly longer delay for reading\n",
    "                stream_data = {\n",
    "                    \"type\": \"stream\",\n",
    "                    \"content\": response,\n",
    "                    \"step\": i + 1,\n",
    "                    \"total_steps\": len(mock_gpt_responses)\n",
    "                }\n",
    "                yield f\"data: {json.dumps(stream_data)}\\n\\n\"\n",
    "            \n",
    "            # Final completion message with detected text\n",
    "            final_data = {\n",
    "                \"type\": \"complete\",\n",
    "                \"content\": \"ğŸ¯ Two Sum solution explained successfully!\\n\",\n",
    "                \"total_frames_processed\": frame_count,\n",
    "                \"detected_text\": f\"\"\"**Detected from {frame_count} frames:**\n",
    "\n",
    "**LeetCode Problem 1: Two Sum**\n",
    "\n",
    "**Problem Statement:**\n",
    "Given an array of integers nums and an integer target, return indices of the two numbers such that they add up to target.\n",
    "\n",
    "You may assume that each input would have exactly one solution, and you may not use the same element twice.\n",
    "\n",
    "**Example 1:**\n",
    "Input: nums = [2,7,11,15], target = 9\n",
    "Output: [0,1]\n",
    "Explanation: Because nums[0] + nums[1] == 9, we return [0, 1].\n",
    "\n",
    "**Example 2:**\n",
    "Input: nums = [3,2,4], target = 6\n",
    "Output: [1,2]\n",
    "\n",
    "**Constraints:**\n",
    "- 2 â‰¤ nums.length â‰¤ 10â´\n",
    "- -10â¹ â‰¤ nums[i] â‰¤ 10â¹\n",
    "- -10â¹ â‰¤ target â‰¤ 10â¹\n",
    "- Only one valid answer exists.\n",
    "\n",
    "**Follow-up:** Can you come up with an algorithm that is less than O(nÂ²) time complexity?\n",
    "\n",
    "**Code Editor shows:**\n",
    "```python\n",
    "class Solution:\n",
    "    def twoSum(self, nums: List[int], target: int) -> List[int]:\n",
    "        # Your solution here\n",
    "        pass\n",
    "```\"\"\"\n",
    "            }\n",
    "            yield f\"data: {json.dumps(final_data)}\\n\\n\"\n",
    "            \n",
    "            # Send the [DONE] signal that frontend is waiting for\n",
    "            yield \"data: [DONE]\\n\\n\"\n",
    "        \n",
    "        return StreamingResponse(\n",
    "            generate_stream(),\n",
    "            media_type=\"text/event-stream\",\n",
    "            headers={\n",
    "                \"Cache-Control\": \"no-cache\",\n",
    "                \"Connection\": \"keep-alive\",\n",
    "                \"Content-Type\": \"text/event-stream\",\n",
    "                \"Access-Control-Allow-Origin\": \"*\",\n",
    "                \"Access-Control-Allow-Methods\": \"*\",\n",
    "                \"Access-Control-Allow-Headers\": \"*\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return JSONResponse({\n",
    "            \"success\": False,\n",
    "            \"error\": f\"Processing failed: {str(e)}\"\n",
    "        })\n",
    "\n",
    "# Start server\n",
    "async def start_frame_saver_server():\n",
    "    import uvicorn\n",
    "    try:\n",
    "        print(\"ğŸš€ Starting FRAME READER & SAVER server on http://localhost:8000\")\n",
    "        print(\"ğŸ“ Frames will be saved to: frames/ directory\")\n",
    "        print(\"ğŸ“¡ Now includes streaming Two Sum solution!\")\n",
    "        \n",
    "        print(\"ğŸ’¡ Send your frontend request to save frames and get streaming output!\")\n",
    "        \n",
    "        config = uvicorn.Config(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
    "        server = uvicorn.Server(config)\n",
    "        await server.serve()\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Server error: {e}\")\n",
    "\n",
    "await start_frame_saver_server()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dfadb7",
   "metadata": {},
   "source": [
    "# ğŸ¤– AI-Powered LeetCode Assistant Server (Production Version)\n",
    "\n",
    "A **production-ready server** that connects to **OpenAI's GPT-4.1-mini** to analyze LeetCode problem screenshots and provide intelligent solutions in real-time.\n",
    "\n",
    "## ğŸ”‘ Prerequisites\n",
    "\n",
    "- OpenAI API key in `.env` file as `OPENAI_API_KEY`\n",
    "- Internet connection for API calls\n",
    "\n",
    "## ğŸ“‹ Endpoint Details\n",
    "\n",
    "**`POST /process-multiple-frames-stream`**\n",
    "\n",
    "**Input:** \n",
    "- Form data with image frames (`frame_0`, `frame_1`, etc.)\n",
    "- Frame count metadata\n",
    "\n",
    "**Output:** \n",
    "- Server-Sent Events stream with real AI analysis\n",
    "- Frames saved to local `frames/` directory  \n",
    "- Live streaming of problem solutions and code explanations\n",
    "\n",
    "## ğŸš€ Usage\n",
    "\n",
    "Set your OpenAI API key, run the server, and send POST request with LeetCode screenshot frames to get real AI-powered analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69c05005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Starting FRAME READER & OPENAI ANALYZER server on http://localhost:8000\n",
      "ğŸ“ Frames will be saved to: frames/ directory\n",
      "ğŸ¤– Now includes real OpenAI GPT-4.1-mini analysis!\n",
      "ğŸ”‘ Make sure your OPENAI_API_KEY is set in .env file\n",
      "ğŸ’¡ Send your frontend request to analyze LeetCode screenshots!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [18776]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "ğŸ”„ READING AND PROCESSING FRAMES...\n",
      "ğŸ“‹ Content-Type: multipart/form-data; boundary=----WebKitFormBoundaryreJ2yR7s3fBdP0SW\n",
      "==================================================\n",
      "ğŸ”„ READING AND PROCESSING FRAMES...\n",
      "ğŸ“‹ Content-Type: multipart/form-data; boundary=----WebKitFormBoundarynx3HJyiKJR1BWIVm\n",
      "ğŸ“¥ Form data items: 14\n",
      "ğŸ” Form keys: ['frame_0', 'frame_1', 'frame_2', 'frame_3', 'frame_4', 'frame_5', 'frame_6', 'frame_7', 'frame_8', 'frame_9', 'frame_10', 'frame_11', 'frame_12', 'frame_count']\n",
      "ğŸ“ Processing frame_0: 1015134 bytes\n",
      "âœ… Saved frame_0: frames/frame_20250717_201429_frame_0.png (1280x720)\n",
      "ğŸ“ Processing frame_1: 994031 bytes\n",
      "âœ… Saved frame_1: frames/frame_20250717_201429_frame_1.png (1280x720)\n",
      "ğŸ“ Processing frame_2: 984846 bytes\n",
      "âœ… Saved frame_2: frames/frame_20250717_201429_frame_2.png (1280x720)\n",
      "ğŸ“ Processing frame_3: 988654 bytes\n",
      "âœ… Saved frame_3: frames/frame_20250717_201429_frame_3.png (1280x720)\n",
      "ğŸ“¥ Form data items: 14\n",
      "ğŸ” Form keys: ['frame_0', 'frame_1', 'frame_2', 'frame_3', 'frame_4', 'frame_5', 'frame_6', 'frame_7', 'frame_8', 'frame_9', 'frame_10', 'frame_11', 'frame_12', 'frame_count']\n",
      "ğŸ“ Processing frame_0: 1015134 bytes\n",
      "âœ… Saved frame_0: frames/frame_20250717_201430_frame_0.png (1280x720)\n",
      "ğŸ“ Processing frame_1: 994031 bytes\n",
      "âœ… Saved frame_1: frames/frame_20250717_201430_frame_1.png (1280x720)\n",
      "ğŸ“ Processing frame_2: 984846 bytes\n",
      "âœ… Saved frame_2: frames/frame_20250717_201430_frame_2.png (1280x720)\n",
      "ğŸ“ Processing frame_3: 988654 bytes\n",
      "âœ… Saved frame_3: frames/frame_20250717_201430_frame_3.png (1280x720)\n",
      "ğŸ“ Processing frame_4: 1063930 bytes\n",
      "âœ… Saved frame_4: frames/frame_20250717_201429_frame_4.png (1280x720)\n",
      "ğŸ“ Processing frame_5: 993181 bytes\n",
      "âœ… Saved frame_5: frames/frame_20250717_201429_frame_5.png (1280x720)\n",
      "ğŸ“ Processing frame_6: 989153 bytes\n",
      "âœ… Saved frame_6: frames/frame_20250717_201429_frame_6.png (1280x720)\n",
      "ğŸ“ Processing frame_7: 933222 bytes\n",
      "âœ… Saved frame_7: frames/frame_20250717_201429_frame_7.png (1280x720)\n",
      "ğŸ“ Processing frame_8: 1006798 bytes\n",
      "âœ… Saved frame_8: frames/frame_20250717_201429_frame_8.png (1280x720)\n",
      "ğŸ“ Processing frame_9: 947384 bytes\n",
      "âœ… Saved frame_9: frames/frame_20250717_201429_frame_9.png (1280x720)\n",
      "ğŸ“ Processing frame_10: 1004530 bytes\n",
      "âœ… Saved frame_10: frames/frame_20250717_201429_frame_10.png (1280x720)\n",
      "ğŸ“ Processing frame_11: 1025513 bytes\n",
      "âœ… Saved frame_11: frames/frame_20250717_201429_frame_11.png (1280x720)\n",
      "ğŸ“ Processing frame_12: 1058722 bytes\n",
      "âœ… Saved frame_12: frames/frame_20250717_201429_frame_12.png (1280x720)\n",
      "ğŸ“Š Expected frame count: 13\n",
      "ğŸ‰ Successfully saved 13 frames to frames/ folder!\n",
      "ğŸ”„ Converting 13 images to base64 for OpenAI API...\n",
      "INFO:     127.0.0.1:51724 - \"POST /process-multiple-frames-stream HTTP/1.1\" 200 OK\n",
      "ğŸ“ Processing frame_4: 1063930 bytes\n",
      "âœ… Saved frame_4: frames/frame_20250717_201430_frame_4.png (1280x720)\n",
      "ğŸ“ Processing frame_5: 993181 bytes\n",
      "âœ… Saved frame_5: frames/frame_20250717_201430_frame_5.png (1280x720)\n",
      "ğŸ“ Processing frame_6: 989153 bytes\n",
      "âœ… Saved frame_6: frames/frame_20250717_201430_frame_6.png (1280x720)\n",
      "ğŸ“ Processing frame_7: 933222 bytes\n",
      "âœ… Saved frame_7: frames/frame_20250717_201430_frame_7.png (1280x720)\n",
      "ğŸ“ Processing frame_8: 1006798 bytes\n",
      "âœ… Saved frame_8: frames/frame_20250717_201430_frame_8.png (1280x720)\n",
      "ğŸ“ Processing frame_9: 947384 bytes\n",
      "âœ… Saved frame_9: frames/frame_20250717_201430_frame_9.png (1280x720)\n",
      "ğŸ“ Processing frame_10: 1004530 bytes\n",
      "âœ… Saved frame_10: frames/frame_20250717_201430_frame_10.png (1280x720)\n",
      "ğŸ“ Processing frame_11: 1025513 bytes\n",
      "âœ… Saved frame_11: frames/frame_20250717_201430_frame_11.png (1280x720)\n",
      "ğŸ¤– Sending 13 images to OpenAI API...\n",
      "ğŸ“ Processing frame_12: 1058722 bytes\n",
      "âœ… Saved frame_12: frames/frame_20250717_201430_frame_12.png (1280x720)\n",
      "ğŸ“Š Expected frame count: 13\n",
      "ğŸ‰ Successfully saved 13 frames to frames/ folder!\n",
      "ğŸ”„ Converting 13 images to base64 for OpenAI API...\n",
      "INFO:     127.0.0.1:51723 - \"POST /process-multiple-frames-stream HTTP/1.1\" 200 OK\n",
      "ğŸ¤– Sending 13 images to OpenAI API...\n",
      "âœ… OpenAI streaming completed!\n",
      "âœ… OpenAI streaming completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Shutting down\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [18776]\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from fastapi import FastAPI, Request\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi.responses import JSONResponse, StreamingResponse\n",
    "from PIL import Image\n",
    "import json\n",
    "import asyncio\n",
    "import io\n",
    "import os\n",
    "import base64\n",
    "from datetime import datetime\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Enable CORS\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "@app.post(\"/process-multiple-frames-stream\")\n",
    "async def simple_frame_reader_with_save(request: Request):\n",
    "    \"\"\"Endpoint that reads frames, saves them, converts to base64, and sends to OpenAI API with streaming response\"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(\"=\" * 50)\n",
    "        print(\"ğŸ”„ READING AND PROCESSING FRAMES...\")\n",
    "        \n",
    "        # Get content type\n",
    "        content_type = request.headers.get(\"content-type\", \"\")\n",
    "        print(f\"ğŸ“‹ Content-Type: {content_type}\")\n",
    "        \n",
    "        # Parse form data\n",
    "        form_data = await request.form()\n",
    "        print(f\"ğŸ“¥ Form data items: {len(form_data)}\")\n",
    "        print(f\"ğŸ” Form keys: {list(form_data.keys())}\")\n",
    "        \n",
    "        # Create frames directory\n",
    "        os.makedirs(\"frames\", exist_ok=True)\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        # Process frames and convert to base64\n",
    "        saved_frames = []\n",
    "        base64_images = []\n",
    "        frame_count = 0\n",
    "        \n",
    "        for key, value in form_data.items():\n",
    "            if key.startswith('frame_') and hasattr(value, 'read'):\n",
    "                try:\n",
    "                    # Read file data\n",
    "                    file_data = await value.read()\n",
    "                    print(f\"ğŸ“ Processing {key}: {len(file_data)} bytes\")\n",
    "                    \n",
    "                    # Open image\n",
    "                    img = Image.open(io.BytesIO(file_data))\n",
    "                    \n",
    "                    # Save frame with timestamp\n",
    "                    save_path = f\"frames/frame_{timestamp}_{key}.png\"\n",
    "                    img.save(save_path)\n",
    "                    \n",
    "                    # Convert to base64 for OpenAI API\n",
    "                    base64_img = base64.b64encode(file_data).decode('utf-8')\n",
    "                    base64_images.append(base64_img)\n",
    "                    \n",
    "                    saved_frames.append({\n",
    "                        \"key\": key,\n",
    "                        \"filename\": getattr(value, 'filename', 'unknown'),\n",
    "                        \"size_bytes\": len(file_data),\n",
    "                        \"image_size\": f\"{img.width}x{img.height}\",\n",
    "                        \"saved_path\": save_path\n",
    "                    })\n",
    "                    \n",
    "                    frame_count += 1\n",
    "                    print(f\"âœ… Saved {key}: {save_path} ({img.width}x{img.height})\")\n",
    "                    \n",
    "                except Exception as frame_error:\n",
    "                    print(f\"âŒ Error processing {key}: {frame_error}\")\n",
    "            \n",
    "            elif key == 'frame_count':\n",
    "                expected_count = str(value)\n",
    "                print(f\"ğŸ“Š Expected frame count: {expected_count}\")\n",
    "        \n",
    "        print(f\"ğŸ‰ Successfully saved {frame_count} frames to frames/ folder!\")\n",
    "        print(f\"ğŸ”„ Converting {len(base64_images)} images to base64 for OpenAI API...\")\n",
    "        \n",
    "        # Now create streaming response with initial success data + real OpenAI streaming\n",
    "        async def generate_stream():\n",
    "            # First yield the success response\n",
    "            initial_response = {\n",
    "                \"success\": True,\n",
    "                \"message\": f\"Successfully received and saved {frame_count} frames!\",\n",
    "                \"frames_saved\": saved_frames,\n",
    "                \"timestamp\": timestamp,\n",
    "                \"save_directory\": \"frames/\",\n",
    "                \"streaming\": True,\n",
    "                \"frame_count\": frame_count,\n",
    "                \"type\": \"initial\"\n",
    "            }\n",
    "            yield f\"data: {json.dumps(initial_response)}\\n\\n\"\n",
    "            \n",
    "            # Small delay before starting analysis\n",
    "            await asyncio.sleep(0.3)\n",
    "            \n",
    "            try:\n",
    "                # Prepare content for OpenAI API\n",
    "                # Prepare content for OpenAI API\n",
    "                content = [\n",
    "                    {\"type\": \"text\", \n",
    "                    \"text\": \"Analyze these LeetCode screenshots captured while scrolling. \"\n",
    "                    \"Different frames may show different parts of the same problem (description, examples, constraints, code editor). \"\n",
    "                    \"Combine information from all frames to provide: 1) Problem name/number 2) Complete working code solution 3) Brief explanation. \"\n",
    "                    \"Be concise - code first, minimal explanation.\"\n",
    "                    \"Ensure not to use any imports or libraries in the code solution\"}\n",
    "                ]\n",
    "                # Add all base64 images to the content\n",
    "                for base64_img in base64_images:\n",
    "                    content.append({\n",
    "                        \"type\": \"image_url\", \n",
    "                        \"image_url\": {\"url\": f\"data:image/png;base64,{base64_img}\"}\n",
    "                    })\n",
    "                \n",
    "                print(f\"ğŸ¤– Sending {len(base64_images)} images to OpenAI API...\")\n",
    "                \n",
    "                # Stream response from OpenAI\n",
    "                stream = client.chat.completions.create(\n",
    "                    model=\"gpt-4.1-mini\",\n",
    "                    messages=[\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": content\n",
    "                        }\n",
    "                    ],\n",
    "                    temperature=0.2,\n",
    "                    stream=True\n",
    "                )\n",
    "                \n",
    "                accumulated_content = \"\"\n",
    "                step = 0\n",
    "                \n",
    "                for chunk in stream:\n",
    "                    # Handle content chunks\n",
    "                    if chunk.choices and chunk.choices[0].delta.content is not None:\n",
    "                        content_chunk = chunk.choices[0].delta.content\n",
    "                        accumulated_content += content_chunk\n",
    "                        step += 1\n",
    "                        \n",
    "                        stream_data = {\n",
    "                            \"type\": \"stream\",\n",
    "                            \"content\": content_chunk,\n",
    "                            \"step\": step,\n",
    "                            \"accumulated\": accumulated_content\n",
    "                        }\n",
    "                        yield f\"data: {json.dumps(stream_data)}\\n\\n\"\n",
    "                        \n",
    "                        # Small delay to make streaming visible\n",
    "                        await asyncio.sleep(0.01)\n",
    "                \n",
    "                print(\"âœ… OpenAI streaming completed!\")\n",
    "                \n",
    "            except Exception as api_error:\n",
    "                print(f\"âŒ OpenAI API Error: {api_error}\")\n",
    "                error_data = {\n",
    "                    \"type\": \"stream\",\n",
    "                    \"content\": f\"âŒ Error calling OpenAI API: {str(api_error)}\\n\\nUsing fallback response...\\n\",\n",
    "                    \"error\": True\n",
    "                }\n",
    "                yield f\"data: {json.dumps(error_data)}\\n\\n\"\n",
    "                \n",
    "                # Fallback message\n",
    "                fallback_data = {\n",
    "                    \"type\": \"stream\",\n",
    "                    \"content\": \"ğŸ¤– Unable to analyze images with AI. Please check your OpenAI API key and try again.\\n\"\n",
    "                }\n",
    "                yield f\"data: {json.dumps(fallback_data)}\\n\\n\"\n",
    "            \n",
    "            # Final completion message\n",
    "            final_data = {\n",
    "                \"type\": \"complete\",\n",
    "                \"content\": \"ğŸ¯ Analysis complete!\\n\",\n",
    "                \"total_frames_processed\": frame_count,\n",
    "                \"detected_text\": f\"\"\"**Placeholder for detected text from {frame_count} frames:**\n",
    "\n",
    "This will be replaced with actual OCR text extraction in future versions.\n",
    "For now, the AI analysis above contains the problem understanding and solution.\n",
    "\n",
    "**Technical Details:**\n",
    "- Frames processed: {frame_count}\n",
    "- Images sent to AI: {len(base64_images)}\n",
    "- Timestamp: {timestamp}\n",
    "- Save location: frames/\n",
    "\n",
    "**Next Steps:**\n",
    "- Implement OCR text extraction\n",
    "- Add text preprocessing\n",
    "- Enhance problem detection accuracy\"\"\"\n",
    "            }\n",
    "            yield f\"data: {json.dumps(final_data)}\\n\\n\"\n",
    "            \n",
    "            # Send the [DONE] signal that frontend is waiting for\n",
    "            yield \"data: [DONE]\\n\\n\"\n",
    "        \n",
    "        return StreamingResponse(\n",
    "            generate_stream(),\n",
    "            media_type=\"text/event-stream\",\n",
    "            headers={\n",
    "                \"Cache-Control\": \"no-cache\",\n",
    "                \"Connection\": \"keep-alive\",\n",
    "                \"Content-Type\": \"text/event-stream\",\n",
    "                \"Access-Control-Allow-Origin\": \"*\",\n",
    "                \"Access-Control-Allow-Methods\": \"*\",\n",
    "                \"Access-Control-Allow-Headers\": \"*\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return JSONResponse({\n",
    "            \"success\": False,\n",
    "            \"error\": f\"Processing failed: {str(e)}\"\n",
    "        })\n",
    "\n",
    "# Start server\n",
    "async def start_frame_saver_server():\n",
    "    import uvicorn\n",
    "    try:\n",
    "        print(\"ğŸš€ Starting FRAME READER & OPENAI ANALYZER server on http://localhost:8000\")\n",
    "        print(\"ğŸ“ Frames will be saved to: frames/ directory\")\n",
    "        print(\"ğŸ¤– Now includes real OpenAI GPT-4.1-mini analysis!\")\n",
    "        print(\"ğŸ”‘ Make sure your OPENAI_API_KEY is set in .env file\")\n",
    "        \n",
    "        print(\"ğŸ’¡ Send your frontend request to analyze LeetCode screenshots!\")\n",
    "        \n",
    "        config = uvicorn.Config(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
    "        server = uvicorn.Server(config)\n",
    "        await server.serve()\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Server error: {e}\")\n",
    "\n",
    "await start_frame_saver_server()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c688dffb",
   "metadata": {},
   "source": [
    "FOR DEVELOPMENT\n",
    "- With Blur check\n",
    "- and simple refinements, increase contrast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11e2177d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Starting DEVELOPMENT FRAME PROCESSOR server on http://localhost:8000\n",
      "ğŸ“ Images organized by session timestamp\n",
      "ğŸ”§ LeetCode-optimized refinement active\n",
      "ğŸ” NOW WITH BLUR DETECTION - filters out blurry images!\n",
      "ğŸ“‚ Directory structure: development/TIMESTAMP/[before_refined|after_refined|rejected_blurry]/\n",
      "ğŸ“‹ Perfect for clustering algorithms with quality control\n",
      "ğŸ’° NO GPT API calls - save costs during development!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [18776]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "ğŸ”„ DEVELOPMENT FRAME PROCESSING WITH BLUR DETECTION...\n",
      "==================================================\n",
      "ğŸ”„ DEVELOPMENT FRAME PROCESSING WITH BLUR DETECTION...\n",
      "ğŸ“¥ Form data items: 14\n",
      "ğŸ“ Processing frame_0: 933115 bytes\n",
      "âŒ REJECTED frame_0 - too blurry (score: 158.42 < 250.0)\n",
      "ğŸ“ Processing frame_1: 910528 bytes\n",
      "âŒ REJECTED frame_1 - too blurry (score: 204.13 < 250.0)\n",
      "ğŸ“ Processing frame_2: 960704 bytes\n",
      "âŒ REJECTED frame_2 - too blurry (score: 216.54 < 250.0)\n",
      "ğŸ“ Processing frame_3: 849002 bytes\n",
      "âŒ REJECTED frame_3 - too blurry (score: 99.64 < 250.0)\n",
      "ğŸ“ Processing frame_4: 883669 bytes\n",
      "âŒ REJECTED frame_4 - too blurry (score: 67.05 < 250.0)\n",
      "ğŸ“ Processing frame_5: 863794 bytes\n",
      "âŒ REJECTED frame_5 - too blurry (score: 94.15 < 250.0)\n",
      "ğŸ“ Processing frame_6: 897859 bytes\n",
      "âŒ REJECTED frame_6 - too blurry (score: 140.47 < 250.0)\n",
      "ğŸ“ Processing frame_7: 952541 bytes\n",
      "âŒ REJECTED frame_7 - too blurry (score: 119.98 < 250.0)\n",
      "ğŸ“ Processing frame_8: 825402 bytes\n",
      "âŒ REJECTED frame_8 - too blurry (score: 71.67 < 250.0)\n",
      "ğŸ“ Processing frame_9: 837299 bytes\n",
      "âŒ REJECTED frame_9 - too blurry (score: 99.92 < 250.0)\n",
      "ğŸ“ Processing frame_10: 805132 bytes\n",
      "âŒ REJECTED frame_10 - too blurry (score: 25.71 < 250.0)\n",
      "ğŸ“ Processing frame_11: 896260 bytes\n",
      "âŒ REJECTED frame_11 - too blurry (score: 102.36 < 250.0)\n",
      "ğŸ“ Processing frame_12: 888054 bytes\n",
      "âŒ REJECTED frame_12 - too blurry (score: 59.34 < 250.0)\n",
      "ğŸ‰ Successfully processed 0 frames!\n",
      "âŒ Rejected 13 blurry frames\n",
      "ğŸ“‹ All 0 sharp images loaded directly into list for clustering\n",
      "INFO:     127.0.0.1:54087 - \"POST /process-multiple-frames-stream HTTP/1.1\" 200 OK\n",
      "ğŸ“¥ Form data items: 14\n",
      "ğŸ“ Processing frame_0: 933115 bytes\n",
      "âŒ REJECTED frame_0 - too blurry (score: 158.42 < 250.0)\n",
      "ğŸ“ Processing frame_1: 910528 bytes\n",
      "âŒ REJECTED frame_1 - too blurry (score: 204.13 < 250.0)\n",
      "ğŸ“ Processing frame_2: 960704 bytes\n",
      "âŒ REJECTED frame_2 - too blurry (score: 216.54 < 250.0)\n",
      "ğŸ“ Processing frame_3: 849002 bytes\n",
      "âŒ REJECTED frame_3 - too blurry (score: 99.64 < 250.0)\n",
      "ğŸ“ Processing frame_4: 883669 bytes\n",
      "âŒ REJECTED frame_4 - too blurry (score: 67.05 < 250.0)\n",
      "ğŸ“ Processing frame_5: 863794 bytes\n",
      "âŒ REJECTED frame_5 - too blurry (score: 94.15 < 250.0)\n",
      "ğŸ“ Processing frame_6: 897859 bytes\n",
      "âŒ REJECTED frame_6 - too blurry (score: 140.47 < 250.0)\n",
      "ğŸ“ Processing frame_7: 952541 bytes\n",
      "âŒ REJECTED frame_7 - too blurry (score: 119.98 < 250.0)\n",
      "ğŸ“ Processing frame_8: 825402 bytes\n",
      "âŒ REJECTED frame_8 - too blurry (score: 71.67 < 250.0)\n",
      "ğŸ“ Processing frame_9: 837299 bytes\n",
      "âŒ REJECTED frame_9 - too blurry (score: 99.92 < 250.0)\n",
      "ğŸ“ Processing frame_10: 805132 bytes\n",
      "âŒ REJECTED frame_10 - too blurry (score: 25.71 < 250.0)\n",
      "ğŸ“ Processing frame_11: 896260 bytes\n",
      "âŒ REJECTED frame_11 - too blurry (score: 102.36 < 250.0)\n",
      "ğŸ“ Processing frame_12: 888054 bytes\n",
      "âŒ REJECTED frame_12 - too blurry (score: 59.34 < 250.0)\n",
      "ğŸ‰ Successfully processed 0 frames!\n",
      "âŒ Rejected 13 blurry frames\n",
      "ğŸ“‹ All 0 sharp images loaded directly into list for clustering\n",
      "INFO:     127.0.0.1:54088 - \"POST /process-multiple-frames-stream HTTP/1.1\" 200 OK\n",
      "==================================================\n",
      "ğŸ”„ DEVELOPMENT FRAME PROCESSING WITH BLUR DETECTION...\n",
      "==================================================\n",
      "ğŸ”„ DEVELOPMENT FRAME PROCESSING WITH BLUR DETECTION...\n",
      "ğŸ“¥ Form data items: 14\n",
      "ğŸ“ Processing frame_0: 1008942 bytes\n",
      "âœ… PASSED blur test frame_0 - score: 381.59\n",
      "âœ… Processed frame_0 - added to images list and saved (blur: 381.59)\n",
      "ğŸ“ Processing frame_1: 948088 bytes\n",
      "âœ… PASSED blur test frame_1 - score: 343.48\n",
      "âœ… Processed frame_1 - added to images list and saved (blur: 343.48)\n",
      "ğŸ“ Processing frame_2: 1037020 bytes\n",
      "âœ… PASSED blur test frame_2 - score: 367.15\n",
      "âœ… Processed frame_2 - added to images list and saved (blur: 367.15)\n",
      "ğŸ“ Processing frame_3: 931948 bytes\n",
      "âœ… PASSED blur test frame_3 - score: 388.23\n",
      "âœ… Processed frame_3 - added to images list and saved (blur: 388.23)\n",
      "ğŸ“ Processing frame_4: 1003713 bytes\n",
      "âœ… PASSED blur test frame_4 - score: 315.33\n",
      "âœ… Processed frame_4 - added to images list and saved (blur: 315.33)\n",
      "ğŸ“ Processing frame_5: 979460 bytes\n",
      "âœ… PASSED blur test frame_5 - score: 256.34\n",
      "âœ… Processed frame_5 - added to images list and saved (blur: 256.34)\n",
      "ğŸ“ Processing frame_6: 1038472 bytes\n",
      "âœ… PASSED blur test frame_6 - score: 347.24\n",
      "âœ… Processed frame_6 - added to images list and saved (blur: 347.24)\n",
      "ğŸ“ Processing frame_7: 944560 bytes\n",
      "âŒ REJECTED frame_7 - too blurry (score: 217.96 < 250.0)\n",
      "ğŸ“ Processing frame_8: 1024923 bytes\n",
      "âŒ REJECTED frame_8 - too blurry (score: 80.02 < 250.0)\n",
      "ğŸ“ Processing frame_9: 909645 bytes\n",
      "âŒ REJECTED frame_9 - too blurry (score: 58.73 < 250.0)\n",
      "ğŸ“ Processing frame_10: 1027129 bytes\n",
      "âŒ REJECTED frame_10 - too blurry (score: 241.77 < 250.0)\n",
      "ğŸ“ Processing frame_11: 1091322 bytes\n",
      "âŒ REJECTED frame_11 - too blurry (score: 243.12 < 250.0)\n",
      "ğŸ“ Processing frame_12: 995147 bytes\n",
      "âŒ REJECTED frame_12 - too blurry (score: 236.11 < 250.0)\n",
      "ğŸ‰ Successfully processed 7 frames!\n",
      "âŒ Rejected 6 blurry frames\n",
      "ğŸ“‹ All 7 sharp images loaded directly into list for clustering\n",
      "INFO:     127.0.0.1:54161 - \"POST /process-multiple-frames-stream HTTP/1.1\" 200 OK\n",
      "ğŸ“¥ Form data items: 14\n",
      "ğŸ“ Processing frame_0: 1008942 bytes\n",
      "âœ… PASSED blur test frame_0 - score: 381.59\n",
      "âœ… Processed frame_0 - added to images list and saved (blur: 381.59)\n",
      "ğŸ“ Processing frame_1: 948088 bytes\n",
      "âœ… PASSED blur test frame_1 - score: 343.48\n",
      "âœ… Processed frame_1 - added to images list and saved (blur: 343.48)\n",
      "ğŸ“ Processing frame_2: 1037020 bytes\n",
      "âœ… PASSED blur test frame_2 - score: 367.15\n",
      "âœ… Processed frame_2 - added to images list and saved (blur: 367.15)\n",
      "ğŸ“ Processing frame_3: 931948 bytes\n",
      "âœ… PASSED blur test frame_3 - score: 388.23\n",
      "âœ… Processed frame_3 - added to images list and saved (blur: 388.23)\n",
      "ğŸ“ Processing frame_4: 1003713 bytes\n",
      "âœ… PASSED blur test frame_4 - score: 315.33\n",
      "âœ… Processed frame_4 - added to images list and saved (blur: 315.33)\n",
      "ğŸ“ Processing frame_5: 979460 bytes\n",
      "âœ… PASSED blur test frame_5 - score: 256.34\n",
      "âœ… Processed frame_5 - added to images list and saved (blur: 256.34)\n",
      "ğŸ“ Processing frame_6: 1038472 bytes\n",
      "âœ… PASSED blur test frame_6 - score: 347.24\n",
      "âœ… Processed frame_6 - added to images list and saved (blur: 347.24)\n",
      "ğŸ“ Processing frame_7: 944560 bytes\n",
      "âŒ REJECTED frame_7 - too blurry (score: 217.96 < 250.0)\n",
      "ğŸ“ Processing frame_8: 1024923 bytes\n",
      "âŒ REJECTED frame_8 - too blurry (score: 80.02 < 250.0)\n",
      "ğŸ“ Processing frame_9: 909645 bytes\n",
      "âŒ REJECTED frame_9 - too blurry (score: 58.73 < 250.0)\n",
      "ğŸ“ Processing frame_10: 1027129 bytes\n",
      "âŒ REJECTED frame_10 - too blurry (score: 241.77 < 250.0)\n",
      "ğŸ“ Processing frame_11: 1091322 bytes\n",
      "âŒ REJECTED frame_11 - too blurry (score: 243.12 < 250.0)\n",
      "ğŸ“ Processing frame_12: 995147 bytes\n",
      "âŒ REJECTED frame_12 - too blurry (score: 236.11 < 250.0)\n",
      "ğŸ‰ Successfully processed 7 frames!\n",
      "âŒ Rejected 6 blurry frames\n",
      "ğŸ“‹ All 7 sharp images loaded directly into list for clustering\n",
      "INFO:     127.0.0.1:54160 - \"POST /process-multiple-frames-stream HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Shutting down\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [18776]\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from fastapi import FastAPI, Request\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi.responses import JSONResponse, StreamingResponse\n",
    "import json\n",
    "import asyncio\n",
    "import io\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import ImageEnhance, ImageFilter,Image\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Enable CORS\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# Enhanced image processing with blur detection\n",
    "class ImageRefiner:\n",
    "    @staticmethod\n",
    "    def placeholder_refinement(img):\n",
    "        \"\"\"Placeholder function - currently returns same image\"\"\"\n",
    "        # For now, just return the same image\n",
    "        # Later you can add actual refinement logic here\n",
    "        return img.copy()\n",
    "    \n",
    "    @staticmethod\n",
    "    def leetcode_optimized_refinement(img):\n",
    "        \"\"\"Improved refinement that reduces glare instead of amplifying it\"\"\"\n",
    "        try:\n",
    "            from PIL import ImageEnhance, ImageFilter\n",
    "            import cv2\n",
    "            import numpy as np\n",
    "            \n",
    "            img_cv = np.array(img)\n",
    "            \n",
    "            # 1. GLARE DETECTION AND REDUCTION FIRST\n",
    "            if len(img_cv.shape) == 3:\n",
    "                # Convert to LAB for better glare handling\n",
    "                lab = cv2.cvtColor(img_cv, cv2.COLOR_RGB2LAB)\n",
    "                l, a, b = cv2.split(lab)\n",
    "                \n",
    "                # Detect overexposed/glare areas (very bright pixels)\n",
    "                glare_mask = cv2.threshold(l, 200, 255, cv2.THRESH_BINARY)[1]\n",
    "                \n",
    "                # Reduce glare by applying bilateral filter to bright areas\n",
    "                l_filtered = cv2.bilateralFilter(l, 9, 80, 80)\n",
    "                \n",
    "                # Blend original and filtered based on glare mask\n",
    "                glare_mask_norm = glare_mask.astype(np.float32) / 255.0\n",
    "                l_reduced = l * (1 - glare_mask_norm) + l_filtered * glare_mask_norm\n",
    "                \n",
    "                # Merge back to RGB\n",
    "                lab_reduced = cv2.merge([l_reduced.astype(np.uint8), a, b])\n",
    "                img_cv = cv2.cvtColor(lab_reduced, cv2.COLOR_LAB2RGB)\n",
    "            \n",
    "            # Convert back to PIL for remaining operations\n",
    "            img_glare_reduced = Image.fromarray(img_cv)\n",
    "            \n",
    "            # 2. GENTLE CONTRAST ENHANCEMENT (much lower)\n",
    "            contrast_enhancer = ImageEnhance.Contrast(img_glare_reduced)\n",
    "            enhanced = contrast_enhancer.enhance(1.1)  # Reduced from 1.4\n",
    "            \n",
    "            # 3. CONDITIONAL BRIGHTNESS - only if image is actually dark\n",
    "            img_array = np.array(enhanced)\n",
    "            mean_brightness = np.mean(img_array)\n",
    "            \n",
    "            if mean_brightness < 120:  # Only brighten if actually dark\n",
    "                brightness_enhancer = ImageEnhance.Brightness(enhanced)\n",
    "                brightened = brightness_enhancer.enhance(1.1)  # Reduced from 1.2\n",
    "            else:\n",
    "                brightened = enhanced  # Skip brightening if already bright enough\n",
    "            \n",
    "            # 4. GENTLE SHARPENING only for text areas\n",
    "            sharpened = brightened.filter(ImageFilter.UnsharpMask(radius=1, percent=120, threshold=3))\n",
    "            \n",
    "            return sharpened\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Refinement failed: {e}\")\n",
    "            return img.copy()\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_blur_score(img):\n",
    "        \"\"\"Calculate blur score using Laplacian variance method\"\"\"\n",
    "        try:\n",
    "            # Convert PIL Image to OpenCV format\n",
    "            img_array = np.array(img)\n",
    "            \n",
    "            # Convert to grayscale if needed\n",
    "            if len(img_array.shape) == 3:\n",
    "                gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)\n",
    "            else:\n",
    "                gray = img_array\n",
    "            \n",
    "            # Calculate Laplacian variance (higher = sharper)\n",
    "            laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
    "            return laplacian_var\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error calculating blur score: {e}\")\n",
    "            return 0.0\n",
    "    \n",
    "    @staticmethod\n",
    "    def is_too_blurry(img, threshold=100.0):\n",
    "        \"\"\"Check if image is too blurry based on threshold\"\"\"\n",
    "        blur_score = ImageRefiner.calculate_blur_score(img)\n",
    "        return blur_score < threshold, blur_score\n",
    "\n",
    "@app.post(\"/process-multiple-frames-stream\")\n",
    "async def process_frames_development(request: Request):\n",
    "    \"\"\"Development endpoint that loads images directly into list for clustering with blur detection\"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(\"=\" * 50)\n",
    "        print(\"ğŸ”„ DEVELOPMENT FRAME PROCESSING WITH BLUR DETECTION...\")\n",
    "        \n",
    "        # Create directory structure: date first, then categories\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        base_dir = \"development\"\n",
    "        session_dir = os.path.join(base_dir, timestamp)\n",
    "        before_dir = os.path.join(session_dir, \"before_refined\")\n",
    "        after_dir = os.path.join(session_dir, \"after_refined\")\n",
    "        rejected_dir = os.path.join(session_dir, \"rejected_blurry\")\n",
    "        \n",
    "        os.makedirs(before_dir, exist_ok=True)\n",
    "        os.makedirs(after_dir, exist_ok=True)\n",
    "        os.makedirs(rejected_dir, exist_ok=True)\n",
    "        \n",
    "        # Parse form data\n",
    "        form_data = await request.form()\n",
    "        print(f\"ğŸ“¥ Form data items: {len(form_data)}\")\n",
    "        \n",
    "        # Lists to store all images directly from form data\n",
    "        all_images = []\n",
    "        processing_results = []\n",
    "        rejected_images = []\n",
    "        frame_count = 0\n",
    "        blur_threshold = 250.0  # Adjust this threshold as needed\n",
    "        \n",
    "        for key, value in form_data.items():\n",
    "            if key.startswith('frame_') and hasattr(value, 'read'):\n",
    "                try:\n",
    "                    # Read file data\n",
    "                    file_data = await value.read()\n",
    "                    print(f\"ğŸ“ Processing {key}: {len(file_data)} bytes\")\n",
    "                    \n",
    "                    # Open original image directly from file data\n",
    "                    original_img = Image.open(io.BytesIO(file_data))\n",
    "                    \n",
    "                    # Check if image is too blurry\n",
    "                    is_blurry, blur_score = ImageRefiner.is_too_blurry(original_img, blur_threshold)\n",
    "                    \n",
    "                    if is_blurry:\n",
    "                        # Save to rejected folder\n",
    "                        rejected_path = os.path.join(rejected_dir, f\"{key}_rejected_blur_{blur_score:.2f}.png\")\n",
    "                        original_img.save(rejected_path)\n",
    "                        \n",
    "                        rejected_images.append({\n",
    "                            \"key\": key,\n",
    "                            \"filename\": getattr(value, 'filename', 'unknown'),\n",
    "                            \"blur_score\": blur_score,\n",
    "                            \"threshold\": blur_threshold,\n",
    "                            \"rejected_path\": rejected_path,\n",
    "                            \"reason\": \"too_blurry\"\n",
    "                        })\n",
    "                        \n",
    "                        print(f\"âŒ REJECTED {key} - too blurry (score: {blur_score:.2f} < {blur_threshold})\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Image passed blur test - process normally\n",
    "                    print(f\"âœ… PASSED blur test {key} - score: {blur_score:.2f}\")\n",
    "                    \n",
    "                    # Apply LeetCode optimized refinement\n",
    "                    refined_img = ImageRefiner.leetcode_optimized_refinement(original_img)\n",
    "                    \n",
    "                    # Add directly to images list for clustering\n",
    "                    image_data = {\n",
    "                        \"key\": key,\n",
    "                        \"original_image\": original_img,\n",
    "                        \"refined_image\": refined_img,\n",
    "                        \"file_data\": file_data,\n",
    "                        \"filename\": getattr(value, 'filename', 'unknown'),\n",
    "                        \"size_bytes\": len(file_data),\n",
    "                        \"image_size\": f\"{original_img.width}x{original_img.height}\",\n",
    "                        \"blur_score\": blur_score\n",
    "                    }\n",
    "                    all_images.append(image_data)\n",
    "                    \n",
    "                    # Save both images (only non-blurry ones)\n",
    "                    before_path = os.path.join(before_dir, f\"{key}_original_blur_{blur_score:.2f}.png\")\n",
    "                    after_path = os.path.join(after_dir, f\"{key}_refined_blur_{blur_score:.2f}.png\")\n",
    "                    \n",
    "                    original_img.save(before_path)\n",
    "                    refined_img.save(after_path)\n",
    "                    \n",
    "                    processing_results.append({\n",
    "                        \"frame_key\": key,\n",
    "                        \"original_path\": before_path,\n",
    "                        \"refined_path\": after_path,\n",
    "                        \"original_size\": f\"{original_img.width}x{original_img.height}\",\n",
    "                        \"original_file_size\": len(file_data),\n",
    "                        \"refined_file_size\": os.path.getsize(after_path),\n",
    "                        \"blur_score\": blur_score,\n",
    "                        \"status\": \"accepted\"\n",
    "                    })\n",
    "                    \n",
    "                    frame_count += 1\n",
    "                    print(f\"âœ… Processed {key} - added to images list and saved (blur: {blur_score:.2f})\")\n",
    "                    \n",
    "                except Exception as frame_error:\n",
    "                    print(f\"âŒ Error processing {key}: {frame_error}\")\n",
    "        \n",
    "        print(f\"ğŸ‰ Successfully processed {frame_count} frames!\")\n",
    "        print(f\"âŒ Rejected {len(rejected_images)} blurry frames\")\n",
    "        print(f\"ğŸ“‹ All {len(all_images)} sharp images loaded directly into list for clustering\")\n",
    "        \n",
    "        # Create streaming response\n",
    "        async def generate_development_stream():\n",
    "            # Initial response\n",
    "            initial_response = {\n",
    "                \"success\": True,\n",
    "                \"message\": f\"Development processing complete! {frame_count} sharp images, {len(rejected_images)} rejected (too blurry)\",\n",
    "                \"timestamp\": timestamp,\n",
    "                \"session_directory\": session_dir,\n",
    "                \"directories\": {\n",
    "                    \"session\": session_dir,\n",
    "                    \"before_refined\": before_dir,\n",
    "                    \"after_refined\": after_dir,\n",
    "                    \"rejected_blurry\": rejected_dir\n",
    "                },\n",
    "                \"frame_count\": frame_count,\n",
    "                \"rejected_count\": len(rejected_images),\n",
    "                \"all_images_count\": len(all_images),\n",
    "                \"blur_threshold\": blur_threshold,\n",
    "                \"type\": \"initial\"\n",
    "            }\n",
    "            yield f\"data: {json.dumps(initial_response)}\\n\\n\"\n",
    "            \n",
    "            await asyncio.sleep(0.3)\n",
    "            \n",
    "            # Stream processing results (accepted images)\n",
    "            for i, result in enumerate(processing_results):\n",
    "                stream_data = {\n",
    "                    \"type\": \"frame_result\",\n",
    "                    \"frame_index\": i + 1,\n",
    "                    \"total_frames\": frame_count,\n",
    "                    \"result\": result\n",
    "                }\n",
    "                yield f\"data: {json.dumps(stream_data)}\\n\\n\"\n",
    "                await asyncio.sleep(0.1)\n",
    "            \n",
    "            # Stream rejected images info\n",
    "            if rejected_images:\n",
    "                await asyncio.sleep(0.2)\n",
    "                for i, rejected in enumerate(rejected_images):\n",
    "                    stream_data = {\n",
    "                        \"type\": \"rejected_frame\",\n",
    "                        \"frame_index\": i + 1,\n",
    "                        \"total_rejected\": len(rejected_images),\n",
    "                        \"rejected\": rejected\n",
    "                    }\n",
    "                    yield f\"data: {json.dumps(stream_data)}\\n\\n\"\n",
    "                    await asyncio.sleep(0.1)\n",
    "            \n",
    "            # Summary with blur detection and clustering info\n",
    "            total_original_size = sum([img[\"size_bytes\"] for img in all_images])\n",
    "            avg_blur_score = sum([img[\"blur_score\"] for img in all_images]) / len(all_images) if all_images else 0\n",
    "            \n",
    "            summary_data = {\n",
    "                \"type\": \"summary\",\n",
    "                \"content\": f\"\"\"ğŸ”¬ **Development Processing Summary with Blur Detection**\n",
    "\n",
    "ğŸ“Š **Frame Statistics:**\n",
    "- Total frames received: {frame_count + len(rejected_images)}\n",
    "- Sharp frames accepted: {frame_count}\n",
    "- Blurry frames rejected: {len(rejected_images)}\n",
    "- Acceptance rate: {(frame_count/(frame_count + len(rejected_images))*100) if (frame_count + len(rejected_images)) > 0 else 0:.1f}%\n",
    "\n",
    "ğŸ“ **Total Size (accepted):** {total_original_size:,} bytes\n",
    "ğŸ“ **Session Directory Structure:**\n",
    "   - Main session: {session_dir}\n",
    "   - Before refinement: {before_dir}\n",
    "   - After refinement: {after_dir}\n",
    "   - Rejected blurry: {rejected_dir}\n",
    "\n",
    "ğŸ” **Blur Detection Results:**\n",
    "- Threshold used: {blur_threshold}\n",
    "- Average blur score (accepted): {avg_blur_score:.2f}\n",
    "- Higher scores = sharper images\n",
    "\n",
    "ğŸ–¼ï¸ **Images Loaded Directly into List:**\n",
    "- Total sharp images: {len(all_images)}\n",
    "- Each image contains: PIL Image objects, metadata, blur scores\n",
    "- Ready for immediate clustering analysis\n",
    "\n",
    "ğŸ”§ **Enhanced Processing Flow:**\n",
    "1. Read file data from form âœ…\n",
    "2. Create PIL Image directly from bytes âœ…\n",
    "3. Calculate blur score using Laplacian variance âœ…\n",
    "4. Filter out blurry images (< {blur_threshold}) âœ…\n",
    "5. Apply LeetCode-optimized refinement âœ…\n",
    "6. Add to all_images list âœ…\n",
    "7. Save to organized directory structure âœ…\n",
    "\n",
    "ğŸ“‚ **Directory Organization:**\n",
    "development/\n",
    "â””â”€â”€ {timestamp}/\n",
    "    â”œâ”€â”€ before_refined/\n",
    "    â”œâ”€â”€ after_refined/\n",
    "    â””â”€â”€ rejected_blurry/\n",
    "\n",
    "ğŸ” **Clustering Ready:**\n",
    "- All {len(all_images)} SHARP images available in `all_images` list\n",
    "- No file I/O needed for clustering algorithms\n",
    "- Direct access to PIL Image objects for feature extraction\n",
    "- Quality assured - no blurry images will affect clustering\n",
    "\n",
    "ğŸ’¡ **Next Steps:**\n",
    "- Implement clustering algorithm on `all_images` list\n",
    "- Feature extraction for similarity analysis\n",
    "- Group similar SHARP frames before GPT processing\n",
    "\n",
    "ğŸš€ **Efficient quality-controlled pipeline** - Only sharp images in memory for clustering!\"\"\",\n",
    "                \"total_original_size\": total_original_size,\n",
    "                \"images_in_memory\": len(all_images),\n",
    "                \"processing_results\": processing_results,\n",
    "                \"rejected_images\": rejected_images,\n",
    "                \"blur_stats\": {\n",
    "                    \"threshold\": blur_threshold,\n",
    "                    \"avg_blur_score\": avg_blur_score,\n",
    "                    \"acceptance_rate\": (frame_count/(frame_count + len(rejected_images))*100) if (frame_count + len(rejected_images)) > 0 else 0\n",
    "                }\n",
    "            }\n",
    "            yield f\"data: {json.dumps(summary_data)}\\n\\n\"\n",
    "            \n",
    "            yield \"data: [DONE]\\n\\n\"\n",
    "        \n",
    "        return StreamingResponse(\n",
    "            generate_development_stream(),\n",
    "            media_type=\"text/event-stream\",\n",
    "            headers={\n",
    "                \"Cache-Control\": \"no-cache\",\n",
    "                \"Connection\": \"keep-alive\",\n",
    "                \"Content-Type\": \"text/event-stream\",\n",
    "                \"Access-Control-Allow-Origin\": \"*\",\n",
    "                \"Access-Control-Allow-Methods\": \"*\",\n",
    "                \"Access-Control-Allow-Headers\": \"*\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return JSONResponse({\n",
    "            \"success\": False,\n",
    "            \"error\": f\"Development processing failed: {str(e)}\"\n",
    "        })\n",
    "\n",
    "# Start server\n",
    "async def start_development_server():\n",
    "    import uvicorn\n",
    "    try:\n",
    "        print(\"ğŸš€ Starting DEVELOPMENT FRAME PROCESSOR server on http://localhost:8000\")\n",
    "        print(\"ğŸ“ Images organized by session timestamp\")\n",
    "        print(\"ğŸ”§ LeetCode-optimized refinement active\")\n",
    "        print(\"ğŸ” NOW WITH BLUR DETECTION - filters out blurry images!\")\n",
    "        print(\"ğŸ“‚ Directory structure: development/TIMESTAMP/[before_refined|after_refined|rejected_blurry]/\")\n",
    "        print(\"ğŸ“‹ Perfect for clustering algorithms with quality control\")\n",
    "        print(\"ğŸ’° NO GPT API calls - save costs during development!\")\n",
    "        \n",
    "        config = uvicorn.Config(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
    "        server = uvicorn.Server(config)\n",
    "        await server.serve()\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Server error: {e}\")\n",
    "\n",
    "await start_development_server()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c6fc53",
   "metadata": {},
   "source": [
    "DEVELOPMENT, exploration on angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b24668a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Starting ENHANCED DEVELOPMENT FRAME PROCESSOR server on http://localhost:8000\n",
      "ğŸ“ Images organized by session timestamp\n",
      "ğŸ”§ LeetCode-optimized refinement with angle correction\n",
      "ğŸ” Blur detection + perspective correction active!\n",
      "ğŸ“ Angle detection using Hough Line Transform\n",
      "ğŸ“‚ Directory: development/TIMESTAMP/[before|after|rejected|angle_analysis]/\n",
      "ğŸ“‹ Perfect for OCR and clustering with geometric correction!\n",
      "ğŸ’° NO GPT API calls - cost-effective development!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [18776]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "ğŸ”„ DEVELOPMENT FRAME PROCESSING WITH BLUR & ANGLE DETECTION...\n",
      "==================================================\n",
      "ğŸ”„ DEVELOPMENT FRAME PROCESSING WITH BLUR & ANGLE DETECTION...\n",
      "ğŸ“¥ Form data items: 14\n",
      "ğŸ“¥ Form data items: 14\n",
      "ğŸ“ Processing frame_0: 1058973 bytes\n",
      "âœ… PASSED tests frame_0 - blur: 600.77, angle: 2.00Â° (success)\n",
      "ğŸ”§ Refinement log: angle_detection: 2.00Â° (success) | angle_correction: corrected_-2.00 | corner_detection: found_6_corners | perspective_correction: skipped | glare_reduction: applied | contrast_enhancement: 1.1x | brightness_enhancement: 1.1x (mean: 110.9) | sharpening: applied\n",
      "âœ… Processed frame_0 - angle improved by -0.50Â°\n",
      "ğŸ“ Processing frame_1: 990503 bytes\n",
      "âœ… PASSED tests frame_1 - blur: 553.84, angle: 1.00Â° (success)\n",
      "ğŸ”§ Refinement log: angle_detection: -1.00Â° (success) | angle_correction: corrected_1.00 | corner_detection: found_10_corners | perspective_correction: skipped | glare_reduction: applied | contrast_enhancement: 1.1x | brightness_enhancement: 1.1x (mean: 119.0) | sharpening: applied\n",
      "âœ… Processed frame_1 - angle improved by -31.00Â°\n",
      "ğŸ“ Processing frame_0: 1058973 bytes\n",
      "âœ… PASSED tests frame_0 - blur: 600.77, angle: 2.00Â° (success)\n",
      "ğŸ”§ Refinement log: angle_detection: 2.00Â° (success) | angle_correction: corrected_-2.00 | corner_detection: found_6_corners | perspective_correction: skipped | glare_reduction: applied | contrast_enhancement: 1.1x | brightness_enhancement: 1.1x (mean: 110.9) | sharpening: applied\n",
      "âœ… Processed frame_0 - angle improved by -0.50Â°\n",
      "ğŸ“ Processing frame_1: 990503 bytes\n",
      "âœ… PASSED tests frame_1 - blur: 553.84, angle: 1.00Â° (success)\n",
      "ğŸ”§ Refinement log: angle_detection: -1.00Â° (success) | angle_correction: corrected_1.00 | corner_detection: found_10_corners | perspective_correction: skipped | glare_reduction: applied | contrast_enhancement: 1.1x | brightness_enhancement: 1.1x (mean: 119.0) | sharpening: applied\n",
      "âœ… Processed frame_1 - angle improved by -31.00Â°\n",
      "ğŸ“ Processing frame_2: 1053108 bytes\n",
      "âœ… PASSED tests frame_2 - blur: 507.89, angle: 3.00Â° (success)\n",
      "ğŸ”§ Refinement log: angle_detection: 3.00Â° (success) | angle_correction: corrected_-3.00 | corner_detection: found_9_corners | perspective_correction: skipped | glare_reduction: applied | contrast_enhancement: 1.1x | brightness_enhancement: skipped (mean: 130.8) | sharpening: applied\n",
      "âœ… Processed frame_2 - angle improved by -1.00Â°\n",
      "ğŸ“ Processing frame_3: 923976 bytes\n",
      "âŒ REJECTED frame_3 - blur: 126.00, angle: 12.00Â°\n",
      "ğŸ“ Processing frame_4: 882447 bytes\n",
      "âŒ REJECTED frame_4 - blur: 82.35, angle: 6.00Â°\n",
      "ğŸ“ Processing frame_5: 838318 bytes\n",
      "âŒ REJECTED frame_5 - blur: 80.62, angle: 4.50Â°\n",
      "ğŸ“ Processing frame_6: 926430 bytes\n",
      "âŒ REJECTED frame_6 - blur: 203.61, angle: 2.50Â°\n",
      "ğŸ“ Processing frame_7: 890110 bytes\n",
      "âŒ REJECTED frame_7 - blur: 207.02, angle: 1.50Â°\n",
      "ğŸ“ Processing frame_8: 866949 bytes\n",
      "âŒ REJECTED frame_8 - blur: 237.98, angle: 5.00Â°\n",
      "ğŸ“ Processing frame_9: 881432 bytes\n",
      "âŒ REJECTED frame_9 - blur: 285.72, angle: 7.00Â°\n",
      "ğŸ“ Processing frame_10: 950408 bytes\n",
      "âŒ REJECTED frame_10 - blur: 199.13, angle: 7.00Â°\n",
      "ğŸ“ Processing frame_11: 926665 bytes\n",
      "âœ… PASSED tests frame_11 - blur: 326.54, angle: 2.00Â° (success)\n",
      "ğŸ”§ Refinement log: angle_detection: 2.00Â° (success) | angle_correction: corrected_-2.00 | corner_detection: found_6_corners | perspective_correction: skipped | glare_reduction: applied | contrast_enhancement: 1.1x | brightness_enhancement: skipped (mean: 125.6) | sharpening: applied\n",
      "âœ… Processed frame_11 - angle improved by -1.00Â°\n",
      "ğŸ“ Processing frame_12: 922771 bytes\n",
      "âŒ REJECTED frame_12 - blur: 178.75, angle: 3.00Â°\n",
      "ğŸ‰ Successfully processed 4 frames!\n",
      "âŒ Rejected 9 blurry frames\n",
      "ğŸ“‹ All 4 sharp images with angle correction ready\n",
      "ğŸ“ Processing frame_2: 1053108 bytes\n",
      "âœ… PASSED tests frame_2 - blur: 507.89, angle: 3.00Â° (success)\n",
      "ğŸ”§ Refinement log: angle_detection: 3.00Â° (success) | angle_correction: corrected_-3.00 | corner_detection: found_9_corners | perspective_correction: skipped | glare_reduction: applied | contrast_enhancement: 1.1x | brightness_enhancement: skipped (mean: 130.8) | sharpening: applied\n",
      "âœ… Processed frame_2 - angle improved by -1.00Â°\n",
      "ğŸ“ Processing frame_3: 923976 bytes\n",
      "âŒ REJECTED frame_3 - blur: 126.00, angle: 12.00Â°\n",
      "ğŸ“ Processing frame_4: 882447 bytes\n",
      "âŒ REJECTED frame_4 - blur: 82.35, angle: 6.00Â°\n",
      "ğŸ“ Processing frame_5: 838318 bytes\n",
      "âŒ REJECTED frame_5 - blur: 80.62, angle: 4.50Â°\n",
      "ğŸ“ Processing frame_6: 926430 bytes\n",
      "âŒ REJECTED frame_6 - blur: 203.61, angle: 2.50Â°\n",
      "ğŸ“ Processing frame_7: 890110 bytes\n",
      "âŒ REJECTED frame_7 - blur: 207.02, angle: 1.50Â°\n",
      "ğŸ“ Processing frame_8: 866949 bytes\n",
      "âŒ REJECTED frame_8 - blur: 237.98, angle: 5.00Â°\n",
      "ğŸ“ Processing frame_9: 881432 bytes\n",
      "âŒ REJECTED frame_9 - blur: 285.72, angle: 7.00Â°\n",
      "ğŸ“ Processing frame_10: 950408 bytes\n",
      "âŒ REJECTED frame_10 - blur: 199.13, angle: 7.00Â°\n",
      "ğŸ“ Processing frame_11: 926665 bytes\n",
      "âœ… PASSED tests frame_11 - blur: 326.54, angle: 2.00Â° (success)\n",
      "ğŸ”§ Refinement log: angle_detection: 2.00Â° (success) | angle_correction: corrected_-2.00 | corner_detection: found_6_corners | perspective_correction: skipped | glare_reduction: applied | contrast_enhancement: 1.1x | brightness_enhancement: skipped (mean: 125.6) | sharpening: applied\n",
      "âœ… Processed frame_11 - angle improved by -1.00Â°\n",
      "ğŸ“ Processing frame_12: 922771 bytes\n",
      "âŒ REJECTED frame_12 - blur: 178.75, angle: 3.00Â°\n",
      "ğŸ‰ Successfully processed 4 frames!\n",
      "âŒ Rejected 9 blurry frames\n",
      "ğŸ“‹ All 4 sharp images with angle correction ready\n",
      "INFO:     127.0.0.1:55410 - \"POST /process-multiple-frames-stream HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:55409 - \"POST /process-multiple-frames-stream HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Shutting down\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [18776]\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from fastapi import FastAPI, Request\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi.responses import JSONResponse, StreamingResponse\n",
    "import json\n",
    "import asyncio\n",
    "import io\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import ImageEnhance, ImageFilter, Image\n",
    "import math\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Enable CORS\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# Enhanced image processing with blur detection and angle correction\n",
    "class ImageRefiner:\n",
    "    @staticmethod\n",
    "    def placeholder_refinement(img):\n",
    "        \"\"\"Placeholder function - currently returns same image\"\"\"\n",
    "        return img.copy()\n",
    "    \n",
    "    @staticmethod\n",
    "    def detect_text_angle(img):\n",
    "        \"\"\"Detect text skew angle using Hough Line Transform\"\"\"\n",
    "        try:\n",
    "            img_array = np.array(img)\n",
    "            \n",
    "            # Convert to grayscale\n",
    "            if len(img_array.shape) == 3:\n",
    "                gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)\n",
    "            else:\n",
    "                gray = img_array\n",
    "            \n",
    "            # Apply edge detection\n",
    "            edges = cv2.Canny(gray, 50, 150, apertureSize=3)\n",
    "            \n",
    "            # Apply Hough Line Transform\n",
    "            lines = cv2.HoughLines(edges, 1, np.pi/180, threshold=100)\n",
    "            \n",
    "            if lines is None:\n",
    "                return 0.0, \"no_lines_detected\"\n",
    "            \n",
    "            # Calculate angles of detected lines\n",
    "            angles = []\n",
    "            for rho, theta in lines[:, 0]:\n",
    "                angle = theta * 180 / np.pi\n",
    "                # Convert to text orientation angle (-90 to 90)\n",
    "                if angle > 90:\n",
    "                    angle = angle - 180\n",
    "                elif angle < -90:\n",
    "                    angle = angle + 180\n",
    "                angles.append(angle)\n",
    "            \n",
    "            # Filter angles close to horizontal (text lines)\n",
    "            horizontal_angles = [a for a in angles if abs(a) < 45]\n",
    "            \n",
    "            if not horizontal_angles:\n",
    "                return 0.0, \"no_horizontal_lines\"\n",
    "            \n",
    "            # Calculate median angle to avoid outliers\n",
    "            median_angle = float(np.median(horizontal_angles))  # Convert to Python float\n",
    "            \n",
    "            return median_angle, \"success\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error detecting angle: {e}\")\n",
    "            return 0.0, f\"error: {str(e)}\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def correct_perspective(img, angle, max_correction=15.0):\n",
    "        \"\"\"Correct image perspective based on detected angle\"\"\"\n",
    "        try:\n",
    "            # Only correct if angle is significant enough\n",
    "            if abs(angle) < 0.5:\n",
    "                return img, f\"angle_too_small_{angle:.2f}\"\n",
    "            \n",
    "            # Limit correction to prevent over-rotation\n",
    "            if abs(angle) > max_correction:\n",
    "                angle = max_correction if angle > 0 else -max_correction\n",
    "                correction_status = f\"limited_to_{angle:.2f}\"\n",
    "            else:\n",
    "                correction_status = f\"corrected_{angle:.2f}\"\n",
    "            \n",
    "            img_array = np.array(img)\n",
    "            height, width = img_array.shape[:2]\n",
    "            \n",
    "            # Calculate rotation matrix\n",
    "            center = (width // 2, height // 2)\n",
    "            rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "            \n",
    "            # Calculate new bounding box to avoid cropping\n",
    "            cos = abs(rotation_matrix[0, 0])\n",
    "            sin = abs(rotation_matrix[0, 1])\n",
    "            new_width = int((height * sin) + (width * cos))\n",
    "            new_height = int((height * cos) + (width * sin))\n",
    "            \n",
    "            # Adjust translation\n",
    "            rotation_matrix[0, 2] += (new_width / 2) - center[0]\n",
    "            rotation_matrix[1, 2] += (new_height / 2) - center[1]\n",
    "            \n",
    "            # Apply rotation\n",
    "            rotated = cv2.warpAffine(img_array, rotation_matrix, (new_width, new_height), \n",
    "                                   flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_CONSTANT, \n",
    "                                   borderValue=(255, 255, 255))\n",
    "            \n",
    "            return Image.fromarray(rotated), correction_status\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error correcting perspective: {e}\")\n",
    "            return img, f\"error: {str(e)}\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def detect_document_corners(img):\n",
    "        \"\"\"Detect document corners for perspective correction (advanced method)\"\"\"\n",
    "        try:\n",
    "            img_array = np.array(img)\n",
    "            \n",
    "            # Convert to grayscale\n",
    "            if len(img_array.shape) == 3:\n",
    "                gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)\n",
    "            else:\n",
    "                gray = img_array\n",
    "            \n",
    "            # Apply Gaussian blur\n",
    "            blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "            \n",
    "            # Apply adaptive threshold\n",
    "            thresh = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \n",
    "                                         cv2.THRESH_BINARY, 11, 2)\n",
    "            \n",
    "            # Find contours\n",
    "            contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            \n",
    "            # Find largest contour (presumably the document)\n",
    "            if not contours:\n",
    "                return None, \"no_contours\"\n",
    "            \n",
    "            largest_contour = max(contours, key=cv2.contourArea)\n",
    "            \n",
    "            # Approximate contour to quadrilateral\n",
    "            epsilon = 0.02 * cv2.arcLength(largest_contour, True)\n",
    "            approx = cv2.approxPolyDP(largest_contour, epsilon, True)\n",
    "            \n",
    "            if len(approx) == 4:\n",
    "                return approx.reshape(4, 2), \"found_corners\"\n",
    "            else:\n",
    "                return None, f\"found_{len(approx)}_corners\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error detecting corners: {e}\")\n",
    "            return None, f\"error: {str(e)}\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def apply_perspective_transform(img, corners):\n",
    "        \"\"\"Apply four-point perspective transform\"\"\"\n",
    "        try:\n",
    "            img_array = np.array(img)\n",
    "            height, width = img_array.shape[:2]\n",
    "            \n",
    "            # Order corners: top-left, top-right, bottom-right, bottom-left\n",
    "            def order_points(pts):\n",
    "                rect = np.zeros((4, 2), dtype=\"float32\")\n",
    "                s = pts.sum(axis=1)\n",
    "                rect[0] = pts[np.argmin(s)]  # top-left\n",
    "                rect[2] = pts[np.argmax(s)]  # bottom-right\n",
    "                diff = np.diff(pts, axis=1)\n",
    "                rect[1] = pts[np.argmin(diff)]  # top-right\n",
    "                rect[3] = pts[np.argmax(diff)]  # bottom-left\n",
    "                return rect\n",
    "            \n",
    "            rect = order_points(corners)\n",
    "            \n",
    "            # Calculate width and height of new image\n",
    "            (tl, tr, br, bl) = rect\n",
    "            widthA = np.sqrt(((br[0] - bl[0]) ** 2) + ((br[1] - bl[1]) ** 2))\n",
    "            widthB = np.sqrt(((tr[0] - tl[0]) ** 2) + ((tr[1] - tl[1]) ** 2))\n",
    "            maxWidth = max(int(widthA), int(widthB))\n",
    "            \n",
    "            heightA = np.sqrt(((tr[0] - br[0]) ** 2) + ((tr[1] - br[1]) ** 2))\n",
    "            heightB = np.sqrt(((tl[0] - bl[0]) ** 2) + ((tl[1] - bl[1]) ** 2))\n",
    "            maxHeight = max(int(heightA), int(heightB))\n",
    "            \n",
    "            # Destination points\n",
    "            dst = np.array([\n",
    "                [0, 0],\n",
    "                [maxWidth - 1, 0],\n",
    "                [maxWidth - 1, maxHeight - 1],\n",
    "                [0, maxHeight - 1]], dtype=\"float32\")\n",
    "            \n",
    "            # Calculate perspective transform matrix\n",
    "            M = cv2.getPerspectiveTransform(rect, dst)\n",
    "            \n",
    "            # Apply perspective transform\n",
    "            warped = cv2.warpPerspective(img_array, M, (maxWidth, maxHeight))\n",
    "            \n",
    "            return Image.fromarray(warped), \"perspective_corrected\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error applying perspective transform: {e}\")\n",
    "            return img, f\"error: {str(e)}\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def leetcode_optimized_refinement(img):\n",
    "        \"\"\"Enhanced refinement with angle detection and perspective correction\"\"\"\n",
    "        try:\n",
    "            refinement_log = []\n",
    "            \n",
    "            # 1. ANGLE DETECTION AND CORRECTION\n",
    "            angle, angle_status = ImageRefiner.detect_text_angle(img)\n",
    "            refinement_log.append(f\"angle_detection: {angle:.2f}Â° ({angle_status})\")\n",
    "            \n",
    "            # Apply angle correction if needed\n",
    "            if abs(angle) > 0.5 and angle_status == \"success\":\n",
    "                img, correction_status = ImageRefiner.correct_perspective(img, -angle)  # Negative to correct\n",
    "                refinement_log.append(f\"angle_correction: {correction_status}\")\n",
    "            else:\n",
    "                refinement_log.append(\"angle_correction: skipped\")\n",
    "            \n",
    "            # 2. DOCUMENT CORNER DETECTION (optional advanced correction)\n",
    "            corners, corner_status = ImageRefiner.detect_document_corners(img)\n",
    "            refinement_log.append(f\"corner_detection: {corner_status}\")\n",
    "            \n",
    "            # Apply perspective correction if corners detected\n",
    "            if corners is not None and corner_status == \"found_corners\":\n",
    "                img, perspective_status = ImageRefiner.apply_perspective_transform(img, corners)\n",
    "                refinement_log.append(f\"perspective_correction: {perspective_status}\")\n",
    "            else:\n",
    "                refinement_log.append(\"perspective_correction: skipped\")\n",
    "            \n",
    "            # 3. CONVERT TO OPENCV FOR PROCESSING\n",
    "            img_cv = np.array(img)\n",
    "            \n",
    "            # 4. GLARE DETECTION AND REDUCTION\n",
    "            if len(img_cv.shape) == 3:\n",
    "                # Convert to LAB for better glare handling\n",
    "                lab = cv2.cvtColor(img_cv, cv2.COLOR_RGB2LAB)\n",
    "                l, a, b = cv2.split(lab)\n",
    "                \n",
    "                # Detect overexposed/glare areas (very bright pixels)\n",
    "                glare_mask = cv2.threshold(l, 200, 255, cv2.THRESH_BINARY)[1]\n",
    "                \n",
    "                # Reduce glare by applying bilateral filter to bright areas\n",
    "                l_filtered = cv2.bilateralFilter(l, 9, 80, 80)\n",
    "                \n",
    "                # Blend original and filtered based on glare mask\n",
    "                glare_mask_norm = glare_mask.astype(np.float32) / 255.0\n",
    "                l_reduced = l * (1 - glare_mask_norm) + l_filtered * glare_mask_norm\n",
    "                \n",
    "                # Merge back to RGB\n",
    "                lab_reduced = cv2.merge([l_reduced.astype(np.uint8), a, b])\n",
    "                img_cv = cv2.cvtColor(lab_reduced, cv2.COLOR_LAB2RGB)\n",
    "                refinement_log.append(\"glare_reduction: applied\")\n",
    "            \n",
    "            # Convert back to PIL for remaining operations\n",
    "            img_glare_reduced = Image.fromarray(img_cv)\n",
    "            \n",
    "            # 5. GENTLE CONTRAST ENHANCEMENT\n",
    "            contrast_enhancer = ImageEnhance.Contrast(img_glare_reduced)\n",
    "            enhanced = contrast_enhancer.enhance(1.1)\n",
    "            refinement_log.append(\"contrast_enhancement: 1.1x\")\n",
    "            \n",
    "            # 6. CONDITIONAL BRIGHTNESS - only if image is actually dark\n",
    "            img_array = np.array(enhanced)\n",
    "            mean_brightness = np.mean(img_array)\n",
    "            \n",
    "            if mean_brightness < 120:\n",
    "                brightness_enhancer = ImageEnhance.Brightness(enhanced)\n",
    "                brightened = brightness_enhancer.enhance(1.1)\n",
    "                refinement_log.append(f\"brightness_enhancement: 1.1x (mean: {mean_brightness:.1f})\")\n",
    "            else:\n",
    "                brightened = enhanced\n",
    "                refinement_log.append(f\"brightness_enhancement: skipped (mean: {mean_brightness:.1f})\")\n",
    "            \n",
    "            # 7. GENTLE SHARPENING for text clarity\n",
    "            sharpened = brightened.filter(ImageFilter.UnsharpMask(radius=1, percent=120, threshold=3))\n",
    "            refinement_log.append(\"sharpening: applied\")\n",
    "            \n",
    "            # Store refinement log in image metadata (for debugging)\n",
    "            if hasattr(sharpened, 'info'):\n",
    "                sharpened.info['refinement_log'] = ' | '.join(refinement_log)\n",
    "            \n",
    "            print(f\"ğŸ”§ Refinement log: {' | '.join(refinement_log)}\")\n",
    "            \n",
    "            return sharpened\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Refinement failed: {e}\")\n",
    "            return img.copy()\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_blur_score(img):\n",
    "        \"\"\"Calculate blur score using Laplacian variance method\"\"\"\n",
    "        try:\n",
    "            # Convert PIL Image to OpenCV format\n",
    "            img_array = np.array(img)\n",
    "            \n",
    "            # Convert to grayscale if needed\n",
    "            if len(img_array.shape) == 3:\n",
    "                gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)\n",
    "            else:\n",
    "                gray = img_array\n",
    "            \n",
    "            # Calculate Laplacian variance (higher = sharper)\n",
    "            laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
    "            return float(laplacian_var)  # Convert to Python float\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error calculating blur score: {e}\")\n",
    "            return 0.0\n",
    "    \n",
    "    @staticmethod\n",
    "    def is_too_blurry(img, threshold=100.0):\n",
    "        \"\"\"Check if image is too blurry based on threshold\"\"\"\n",
    "        blur_score = ImageRefiner.calculate_blur_score(img)\n",
    "        return blur_score < threshold, blur_score\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_angle_score(img):\n",
    "        \"\"\"Calculate how tilted the image is\"\"\"\n",
    "        try:\n",
    "            angle, status = ImageRefiner.detect_text_angle(img)\n",
    "            return float(abs(angle)), status  # Convert to Python float\n",
    "        except Exception as e:\n",
    "            return 0.0, f\"error: {str(e)}\"\n",
    "\n",
    "# Helper function to convert numpy types to Python types for JSON serialization\n",
    "def convert_to_json_serializable(obj):\n",
    "    \"\"\"Convert numpy types to Python types for JSON serialization\"\"\"\n",
    "    if isinstance(obj, np.integer):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, dict):\n",
    "        return {key: convert_to_json_serializable(value) for key, value in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_json_serializable(item) for item in obj]\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "@app.post(\"/process-multiple-frames-stream\")\n",
    "async def process_frames_development(request: Request):\n",
    "    \"\"\"Development endpoint with blur detection, angle detection, and perspective correction\"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(\"=\" * 50)\n",
    "        print(\"ğŸ”„ DEVELOPMENT FRAME PROCESSING WITH BLUR & ANGLE DETECTION...\")\n",
    "        \n",
    "        # Create directory structure\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        base_dir = \"development\"\n",
    "        session_dir = os.path.join(base_dir, timestamp)\n",
    "        before_dir = os.path.join(session_dir, \"before_refined\")\n",
    "        after_dir = os.path.join(session_dir, \"after_refined\")\n",
    "        rejected_dir = os.path.join(session_dir, \"rejected_blurry\")\n",
    "        angle_dir = os.path.join(session_dir, \"angle_analysis\")\n",
    "        \n",
    "        os.makedirs(before_dir, exist_ok=True)\n",
    "        os.makedirs(after_dir, exist_ok=True)\n",
    "        os.makedirs(rejected_dir, exist_ok=True)\n",
    "        os.makedirs(angle_dir, exist_ok=True)\n",
    "        \n",
    "        # Parse form data\n",
    "        form_data = await request.form()\n",
    "        print(f\"ğŸ“¥ Form data items: {len(form_data)}\")\n",
    "        \n",
    "        # Lists to store all images\n",
    "        all_images = []\n",
    "        processing_results = []\n",
    "        rejected_images = []\n",
    "        frame_count = 0\n",
    "        blur_threshold = 300.0\n",
    "        \n",
    "        for key, value in form_data.items():\n",
    "            if key.startswith('frame_') and hasattr(value, 'read'):\n",
    "                try:\n",
    "                    # Read file data\n",
    "                    file_data = await value.read()\n",
    "                    print(f\"ğŸ“ Processing {key}: {len(file_data)} bytes\")\n",
    "                    \n",
    "                    # Open original image\n",
    "                    original_img = Image.open(io.BytesIO(file_data))\n",
    "                    \n",
    "                    # Check blur score\n",
    "                    is_blurry, blur_score = ImageRefiner.is_too_blurry(original_img, blur_threshold)\n",
    "                    \n",
    "                    # Check angle\n",
    "                    angle_score, angle_status = ImageRefiner.calculate_angle_score(original_img)\n",
    "                    \n",
    "                    if is_blurry:\n",
    "                        # Save to rejected folder\n",
    "                        rejected_path = os.path.join(rejected_dir, f\"{key}_rejected_blur_{blur_score:.2f}_angle_{angle_score:.2f}.png\")\n",
    "                        original_img.save(rejected_path)\n",
    "                        \n",
    "                        rejected_images.append({\n",
    "                            \"key\": key,\n",
    "                            \"filename\": getattr(value, 'filename', 'unknown'),\n",
    "                            \"blur_score\": float(blur_score),  # Ensure Python float\n",
    "                            \"angle_score\": float(angle_score),  # Ensure Python float\n",
    "                            \"angle_status\": angle_status,\n",
    "                            \"threshold\": float(blur_threshold),  # Ensure Python float\n",
    "                            \"rejected_path\": rejected_path,\n",
    "                            \"reason\": \"too_blurry\"\n",
    "                        })\n",
    "                        \n",
    "                        print(f\"âŒ REJECTED {key} - blur: {blur_score:.2f}, angle: {angle_score:.2f}Â°\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Image passed tests - process normally\n",
    "                    print(f\"âœ… PASSED tests {key} - blur: {blur_score:.2f}, angle: {angle_score:.2f}Â° ({angle_status})\")\n",
    "                    \n",
    "                    # Apply enhanced refinement with angle correction\n",
    "                    refined_img = ImageRefiner.leetcode_optimized_refinement(original_img)\n",
    "                    \n",
    "                    # Calculate post-refinement angle for comparison\n",
    "                    post_angle, post_angle_status = ImageRefiner.calculate_angle_score(refined_img)\n",
    "                    \n",
    "                    # Add to images list\n",
    "                    image_data = {\n",
    "                        \"key\": key,\n",
    "                        \"original_image\": original_img,\n",
    "                        \"refined_image\": refined_img,\n",
    "                        \"file_data\": file_data,\n",
    "                        \"filename\": getattr(value, 'filename', 'unknown'),\n",
    "                        \"size_bytes\": len(file_data),\n",
    "                        \"image_size\": f\"{original_img.width}x{original_img.height}\",\n",
    "                        \"blur_score\": float(blur_score),  # Ensure Python float\n",
    "                        \"original_angle\": float(angle_score),  # Ensure Python float\n",
    "                        \"refined_angle\": float(post_angle),  # Ensure Python float\n",
    "                        \"angle_status\": angle_status,\n",
    "                        \"angle_improvement\": float(angle_score - post_angle)  # Ensure Python float\n",
    "                    }\n",
    "                    all_images.append(image_data)\n",
    "                    \n",
    "                    # Save images with detailed naming\n",
    "                    before_path = os.path.join(before_dir, f\"{key}_original_blur_{blur_score:.2f}_angle_{angle_score:.2f}.png\")\n",
    "                    after_path = os.path.join(after_dir, f\"{key}_refined_blur_{blur_score:.2f}_angle_{post_angle:.2f}.png\")\n",
    "                    \n",
    "                    original_img.save(before_path)\n",
    "                    refined_img.save(after_path)\n",
    "                    \n",
    "                    # Create angle analysis visualization\n",
    "                    angle_analysis_path = os.path.join(angle_dir, f\"{key}_angle_analysis.png\")\n",
    "                    # Save side-by-side comparison\n",
    "                    combined_width = original_img.width + refined_img.width\n",
    "                    combined_height = max(original_img.height, refined_img.height)\n",
    "                    combined_img = Image.new('RGB', (combined_width, combined_height), (255, 255, 255))\n",
    "                    combined_img.paste(original_img, (0, 0))\n",
    "                    combined_img.paste(refined_img, (original_img.width, 0))\n",
    "                    combined_img.save(angle_analysis_path)\n",
    "                    \n",
    "                    processing_results.append({\n",
    "                        \"frame_key\": key,\n",
    "                        \"original_path\": before_path,\n",
    "                        \"refined_path\": after_path,\n",
    "                        \"angle_analysis_path\": angle_analysis_path,\n",
    "                        \"original_size\": f\"{original_img.width}x{original_img.height}\",\n",
    "                        \"original_file_size\": len(file_data),\n",
    "                        \"refined_file_size\": os.path.getsize(after_path),\n",
    "                        \"blur_score\": float(blur_score),  # Ensure Python float\n",
    "                        \"original_angle\": float(angle_score),  # Ensure Python float\n",
    "                        \"refined_angle\": float(post_angle),  # Ensure Python float\n",
    "                        \"angle_improvement\": float(angle_score - post_angle),  # Ensure Python float\n",
    "                        \"angle_status\": angle_status,\n",
    "                        \"status\": \"accepted\"\n",
    "                    })\n",
    "                    \n",
    "                    frame_count += 1\n",
    "                    print(f\"âœ… Processed {key} - angle improved by {angle_score - post_angle:.2f}Â°\")\n",
    "                    \n",
    "                except Exception as frame_error:\n",
    "                    print(f\"âŒ Error processing {key}: {frame_error}\")\n",
    "        \n",
    "        print(f\"ğŸ‰ Successfully processed {frame_count} frames!\")\n",
    "        print(f\"âŒ Rejected {len(rejected_images)} blurry frames\")\n",
    "        print(f\"ğŸ“‹ All {len(all_images)} sharp images with angle correction ready\")\n",
    "        \n",
    "        # Create streaming response\n",
    "        async def generate_development_stream():\n",
    "            # Initial response\n",
    "            initial_response = {\n",
    "                \"success\": True,\n",
    "                \"message\": f\"Development processing complete! {frame_count} sharp images, {len(rejected_images)} rejected\",\n",
    "                \"timestamp\": timestamp,\n",
    "                \"session_directory\": session_dir,\n",
    "                \"directories\": {\n",
    "                    \"session\": session_dir,\n",
    "                    \"before_refined\": before_dir,\n",
    "                    \"after_refined\": after_dir,\n",
    "                    \"rejected_blurry\": rejected_dir,\n",
    "                    \"angle_analysis\": angle_dir\n",
    "                },\n",
    "                \"frame_count\": frame_count,\n",
    "                \"rejected_count\": len(rejected_images),\n",
    "                \"all_images_count\": len(all_images),\n",
    "                \"blur_threshold\": float(blur_threshold),  # Ensure Python float\n",
    "                \"type\": \"initial\"\n",
    "            }\n",
    "            yield f\"data: {json.dumps(convert_to_json_serializable(initial_response))}\\n\\n\"\n",
    "            \n",
    "            await asyncio.sleep(0.3)\n",
    "            \n",
    "            # Stream processing results\n",
    "            for i, result in enumerate(processing_results):\n",
    "                stream_data = {\n",
    "                    \"type\": \"frame_result\",\n",
    "                    \"frame_index\": i + 1,\n",
    "                    \"total_frames\": frame_count,\n",
    "                    \"result\": convert_to_json_serializable(result)\n",
    "                }\n",
    "                yield f\"data: {json.dumps(stream_data)}\\n\\n\"\n",
    "                await asyncio.sleep(0.1)\n",
    "            \n",
    "            # Stream rejected images info\n",
    "            if rejected_images:\n",
    "                await asyncio.sleep(0.2)\n",
    "                for i, rejected in enumerate(rejected_images):\n",
    "                    stream_data = {\n",
    "                        \"type\": \"rejected_frame\",\n",
    "                        \"frame_index\": i + 1,\n",
    "                        \"total_rejected\": len(rejected_images),\n",
    "                        \"rejected\": convert_to_json_serializable(rejected)\n",
    "                    }\n",
    "                    yield f\"data: {json.dumps(stream_data)}\\n\\n\"\n",
    "                    await asyncio.sleep(0.1)\n",
    "            \n",
    "            # Calculate statistics\n",
    "            total_original_size = sum([img[\"size_bytes\"] for img in all_images])\n",
    "            avg_blur_score = sum([img[\"blur_score\"] for img in all_images]) / len(all_images) if all_images else 0\n",
    "            avg_angle_improvement = sum([img[\"angle_improvement\"] for img in all_images]) / len(all_images) if all_images else 0\n",
    "            \n",
    "            summary_data = {\n",
    "                \"type\": \"summary\",\n",
    "                \"content\": f\"\"\"ğŸ”¬ **Development Processing Summary with Blur & Angle Detection**\n",
    "\n",
    "ğŸ“Š **Frame Statistics:**\n",
    "- Total frames received: {frame_count + len(rejected_images)}\n",
    "- Sharp frames accepted: {frame_count}\n",
    "- Blurry frames rejected: {len(rejected_images)}\n",
    "- Acceptance rate: {(frame_count/(frame_count + len(rejected_images))*100) if (frame_count + len(rejected_images)) > 0 else 0:.1f}%\n",
    "\n",
    "ğŸ“ **Total Size (accepted):** {total_original_size:,} bytes\n",
    "ğŸ“ **Session Directory Structure:**\n",
    "   - Main session: {session_dir}\n",
    "   - Before refinement: {before_dir}\n",
    "   - After refinement: {after_dir}\n",
    "   - Rejected blurry: {rejected_dir}\n",
    "   - Angle analysis: {angle_dir}\n",
    "\n",
    "ğŸ” **Quality Control Results:**\n",
    "- Blur threshold: {blur_threshold}\n",
    "- Average blur score: {avg_blur_score:.2f}\n",
    "- Average angle improvement: {avg_angle_improvement:.2f}Â°\n",
    "\n",
    "ğŸ–¼ï¸ **Enhanced Processing Features:**\n",
    "- âœ… Blur detection and filtering\n",
    "- âœ… Text angle detection using Hough transforms\n",
    "- âœ… Automatic perspective correction\n",
    "- âœ… Document corner detection (advanced)\n",
    "- âœ… Glare reduction in LAB color space\n",
    "- âœ… Conditional brightness adjustment\n",
    "- âœ… Gentle contrast enhancement\n",
    "- âœ… Text-optimized sharpening\n",
    "\n",
    "ğŸ”§ **Angle Correction Pipeline:**\n",
    "1. Detect text lines using Canny edge detection\n",
    "2. Apply Hough Line Transform to find line angles\n",
    "3. Calculate median angle for robustness\n",
    "4. Correct perspective if angle > 0.5Â°\n",
    "5. Optional: Four-point perspective correction\n",
    "6. Apply image enhancement pipeline\n",
    "\n",
    "ğŸ“‚ **Directory Organization:**\n",
    "development/\n",
    "â””â”€â”€ {timestamp}/\n",
    "    â”œâ”€â”€ before_refined/ (original images)\n",
    "    â”œâ”€â”€ after_refined/ (enhanced images)\n",
    "    â”œâ”€â”€ rejected_blurry/ (filtered out)\n",
    "    â””â”€â”€ angle_analysis/ (side-by-side comparisons)\n",
    "\n",
    "ğŸ” **Clustering Ready:**\n",
    "- All {len(all_images)} SHARP, STRAIGHT images available\n",
    "- Enhanced image quality for better OCR\n",
    "- Perspective-corrected for optimal text recognition\n",
    "- Quality metadata for clustering algorithms\n",
    "\n",
    "ğŸ’¡ **Next Steps:**\n",
    "- Implement clustering on corrected images\n",
    "- Add OCR text extraction pipeline\n",
    "- Feature extraction for similarity analysis\n",
    "\n",
    "ğŸš€ **Quality-controlled pipeline with geometric correction!**\"\"\",\n",
    "                \"total_original_size\": total_original_size,\n",
    "                \"images_in_memory\": len(all_images),\n",
    "                \"processing_results\": convert_to_json_serializable(processing_results),\n",
    "                \"rejected_images\": convert_to_json_serializable(rejected_images),\n",
    "                \"quality_stats\": {\n",
    "                    \"blur_threshold\": float(blur_threshold),\n",
    "                    \"avg_blur_score\": float(avg_blur_score),\n",
    "                    \"avg_angle_improvement\": float(avg_angle_improvement),\n",
    "                    \"acceptance_rate\": float((frame_count/(frame_count + len(rejected_images))*100) if (frame_count + len(rejected_images)) > 0 else 0)\n",
    "                }\n",
    "            }\n",
    "            yield f\"data: {json.dumps(convert_to_json_serializable(summary_data))}\\n\\n\"\n",
    "            \n",
    "            yield \"data: [DONE]\\n\\n\"\n",
    "        \n",
    "        return StreamingResponse(\n",
    "            generate_development_stream(),\n",
    "            media_type=\"text/event-stream\",\n",
    "            headers={\n",
    "                \"Cache-Control\": \"no-cache\",\n",
    "                \"Connection\": \"keep-alive\",\n",
    "                \"Content-Type\": \"text/event-stream\",\n",
    "                \"Access-Control-Allow-Origin\": \"*\",\n",
    "                \"Access-Control-Allow-Methods\": \"*\",\n",
    "                \"Access-Control-Allow-Headers\": \"*\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return JSONResponse({\n",
    "            \"success\": False,\n",
    "            \"error\": f\"Development processing failed: {str(e)}\"\n",
    "        })\n",
    "\n",
    "# Start server\n",
    "async def start_development_server():\n",
    "    import uvicorn\n",
    "    try:\n",
    "        print(\"ğŸš€ Starting ENHANCED DEVELOPMENT FRAME PROCESSOR server on http://localhost:8000\")\n",
    "        print(\"ğŸ“ Images organized by session timestamp\")\n",
    "        print(\"ğŸ”§ LeetCode-optimized refinement with angle correction\")\n",
    "        print(\"ğŸ” Blur detection + perspective correction active!\")\n",
    "        print(\"ğŸ“ Angle detection using Hough Line Transform\")\n",
    "        print(\"ğŸ“‚ Directory: development/TIMESTAMP/[before|after|rejected|angle_analysis]/\")\n",
    "        print(\"ğŸ“‹ Perfect for OCR and clustering with geometric correction!\")\n",
    "        print(\"ğŸ’° NO GPT API calls - cost-effective development!\")\n",
    "        \n",
    "        config = uvicorn.Config(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
    "        server = uvicorn.Server(config)\n",
    "        await server.serve()\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Server error: {e}\")\n",
    "\n",
    "await start_development_server()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MIT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
