{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24211645",
   "metadata": {},
   "source": [
    "# ğŸ§ª Mock LeetCode Assistant Server (Testing Version)\n",
    "\n",
    "A **hardcoded testing server** that simulates LeetCode problem analysis without connecting to OpenAI's API. Returns a pre-written Two Sum solution for development and testing purposes.\n",
    "\n",
    "## ğŸ“‹ Endpoint Details\n",
    "\n",
    "**`POST /process-multiple-frames-stream`**\n",
    "\n",
    "**Input:** \n",
    "- Form data with image frames (`frame_0`, `frame_1`, etc.)\n",
    "- Frame count metadata\n",
    "\n",
    "**Output:** \n",
    "- Server-Sent Events stream with mock Two Sum solution\n",
    "- Frames saved to local `frames/` directory\n",
    "- Mock detected text and problem explanation\n",
    "\n",
    "## ğŸš€ Usage\n",
    "\n",
    "Run the server and send POST request with image frames to get streaming mock AI response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "173fb8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [21908]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Starting FRAME READER & SAVER server on http://localhost:8000\n",
      "ğŸ“ Frames will be saved to: frames/ directory\n",
      "ğŸ“¡ Now includes streaming Two Sum solution!\n",
      "ğŸ’¡ Send your frontend request to save frames and get streaming output!\n",
      "==================================================\n",
      "ğŸ”„ READING AND SAVING FRAMES...\n",
      "ğŸ“‹ Content-Type: multipart/form-data; boundary=----WebKitFormBoundary4kuMiTruikFaqJ7b\n",
      "==================================================\n",
      "ğŸ”„ READING AND SAVING FRAMES...\n",
      "ğŸ“‹ Content-Type: multipart/form-data; boundary=----WebKitFormBoundaryUSNpc3M7s1v046aq\n",
      "ğŸ“¥ Form data items: 5\n",
      "ğŸ” Form keys: ['frame_0', 'frame_1', 'frame_2', 'frame_3', 'frame_count']\n",
      "ğŸ“¥ Form data items: 5\n",
      "ğŸ” Form keys: ['frame_0', 'frame_1', 'frame_2', 'frame_3', 'frame_count']\n",
      "ğŸ“ Processing frame_0: 1180743 bytes\n",
      "âœ… Saved frame_0: frames/frame_20250709_122518_frame_0.png (1280x720)\n",
      "ğŸ“ Processing frame_0: 1180743 bytes\n",
      "âœ… Saved frame_0: frames/frame_20250709_122518_frame_0.png (1280x720)\n",
      "ğŸ“ Processing frame_1: 1166641 bytes\n",
      "âœ… Saved frame_1: frames/frame_20250709_122518_frame_1.png (1280x720)\n",
      "ğŸ“ Processing frame_2: 1182030 bytes\n",
      "âœ… Saved frame_2: frames/frame_20250709_122518_frame_2.png (1280x720)\n",
      "ğŸ“ Processing frame_1: 1166641 bytes\n",
      "âœ… Saved frame_1: frames/frame_20250709_122518_frame_1.png (1280x720)\n",
      "ğŸ“ Processing frame_2: 1182030 bytes\n",
      "âœ… Saved frame_2: frames/frame_20250709_122518_frame_2.png (1280x720)\n",
      "ğŸ“ Processing frame_3: 1178373 bytes\n",
      "âœ… Saved frame_3: frames/frame_20250709_122518_frame_3.png (1280x720)\n",
      "ğŸ“Š Expected frame count: 4\n",
      "ğŸ‰ Successfully saved 4 frames to frames/ folder!\n",
      "INFO:     127.0.0.1:57640 - \"POST /process-multiple-frames-stream HTTP/1.1\" 200 OK\n",
      "ğŸ“ Processing frame_3: 1178373 bytes\n",
      "âœ… Saved frame_3: frames/frame_20250709_122518_frame_3.png (1280x720)\n",
      "ğŸ“Š Expected frame count: 4\n",
      "ğŸ‰ Successfully saved 4 frames to frames/ folder!\n",
      "INFO:     127.0.0.1:57641 - \"POST /process-multiple-frames-stream HTTP/1.1\" 200 OK\n",
      "==================================================\n",
      "ğŸ”„ READING AND SAVING FRAMES...\n",
      "ğŸ“‹ Content-Type: multipart/form-data; boundary=----WebKitFormBoundaryqki1UfYCWQLMb4pT\n",
      "ğŸ“¥ Form data items: 5\n",
      "ğŸ” Form keys: ['frame_0', 'frame_1', 'frame_2', 'frame_3', 'frame_count']\n",
      "ğŸ“ Processing frame_0: 1174859 bytes\n",
      "âœ… Saved frame_0: frames/frame_20250709_122614_frame_0.png (1280x720)\n",
      "ğŸ“ Processing frame_1: 1175226 bytes\n",
      "âœ… Saved frame_1: frames/frame_20250709_122614_frame_1.png (1280x720)\n",
      "==================================================\n",
      "ğŸ”„ READING AND SAVING FRAMES...\n",
      "ğŸ“‹ Content-Type: multipart/form-data; boundary=----WebKitFormBoundary6BW8XpnPBhuGqh6J\n",
      "ğŸ“ Processing frame_2: 1171640 bytes\n",
      "âœ… Saved frame_2: frames/frame_20250709_122614_frame_2.png (1280x720)\n",
      "ğŸ“ Processing frame_3: 1174039 bytes\n",
      "âœ… Saved frame_3: frames/frame_20250709_122614_frame_3.png (1280x720)\n",
      "ğŸ“Š Expected frame count: 4\n",
      "ğŸ‰ Successfully saved 4 frames to frames/ folder!\n",
      "INFO:     127.0.0.1:57642 - \"POST /process-multiple-frames-stream HTTP/1.1\" 200 OK\n",
      "ğŸ“¥ Form data items: 5\n",
      "ğŸ” Form keys: ['frame_0', 'frame_1', 'frame_2', 'frame_3', 'frame_count']\n",
      "ğŸ“ Processing frame_0: 1174859 bytes\n",
      "âœ… Saved frame_0: frames/frame_20250709_122615_frame_0.png (1280x720)\n",
      "ğŸ“ Processing frame_1: 1175226 bytes\n",
      "âœ… Saved frame_1: frames/frame_20250709_122615_frame_1.png (1280x720)\n",
      "ğŸ“ Processing frame_2: 1171640 bytes\n",
      "âœ… Saved frame_2: frames/frame_20250709_122615_frame_2.png (1280x720)\n",
      "ğŸ“ Processing frame_3: 1174039 bytes\n",
      "âœ… Saved frame_3: frames/frame_20250709_122615_frame_3.png (1280x720)\n",
      "ğŸ“Š Expected frame count: 4\n",
      "ğŸ‰ Successfully saved 4 frames to frames/ folder!\n",
      "INFO:     127.0.0.1:57685 - \"POST /process-multiple-frames-stream HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Shutting down\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [21908]\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from fastapi import FastAPI, Request\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi.responses import JSONResponse, StreamingResponse\n",
    "from PIL import Image\n",
    "import json\n",
    "import asyncio\n",
    "import io\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Enable CORS\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "@app.post(\"/process-multiple-frames-stream\")\n",
    "async def simple_frame_reader_with_save(request: Request):\n",
    "    \"\"\"Simple endpoint that reads frames AND saves them to frames folder, then streams text\"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(\"=\" * 50)\n",
    "        print(\"ğŸ”„ READING AND SAVING FRAMES...\")\n",
    "        \n",
    "        # Get content type\n",
    "        content_type = request.headers.get(\"content-type\", \"\")\n",
    "        print(f\"ğŸ“‹ Content-Type: {content_type}\")\n",
    "        \n",
    "        # Parse form data\n",
    "        form_data = await request.form()\n",
    "        print(f\"ğŸ“¥ Form data items: {len(form_data)}\")\n",
    "        print(f\"ğŸ” Form keys: {list(form_data.keys())}\")\n",
    "        \n",
    "        # Create frames directory\n",
    "        os.makedirs(\"frames\", exist_ok=True)\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        # Process and save each frame\n",
    "        saved_frames = []\n",
    "        frame_count = 0\n",
    "        \n",
    "        for key, value in form_data.items():\n",
    "            if key.startswith('frame_') and hasattr(value, 'read'):\n",
    "                try:\n",
    "                    # Read file data\n",
    "                    file_data = await value.read()\n",
    "                    print(f\"ğŸ“ Processing {key}: {len(file_data)} bytes\")\n",
    "                    \n",
    "                    # Open image\n",
    "                    img = Image.open(io.BytesIO(file_data))\n",
    "                    \n",
    "                    # Save frame with timestamp\n",
    "                    save_path = f\"frames/frame_{timestamp}_{key}.png\"\n",
    "                    img.save(save_path)\n",
    "                    \n",
    "                    saved_frames.append({\n",
    "                        \"key\": key,\n",
    "                        \"filename\": getattr(value, 'filename', 'unknown'),\n",
    "                        \"size_bytes\": len(file_data),\n",
    "                        \"image_size\": f\"{img.width}x{img.height}\",\n",
    "                        \"saved_path\": save_path\n",
    "                    })\n",
    "                    \n",
    "                    frame_count += 1\n",
    "                    print(f\"âœ… Saved {key}: {save_path} ({img.width}x{img.height})\")\n",
    "                    \n",
    "                except Exception as frame_error:\n",
    "                    print(f\"âŒ Error processing {key}: {frame_error}\")\n",
    "            \n",
    "            elif key == 'frame_count':\n",
    "                expected_count = str(value)\n",
    "                print(f\"ğŸ“Š Expected frame count: {expected_count}\")\n",
    "        \n",
    "        print(f\"ğŸ‰ Successfully saved {frame_count} frames to frames/ folder!\")\n",
    "        \n",
    "        # Now create streaming response with initial success data + streaming text\n",
    "        async def generate_stream():\n",
    "            # First yield the success response\n",
    "            initial_response = {\n",
    "                \"success\": True,\n",
    "                \"message\": f\"Successfully received and saved {frame_count} frames!\",\n",
    "                \"frames_saved\": saved_frames,\n",
    "                \"timestamp\": timestamp,\n",
    "                \"save_directory\": \"frames/\",\n",
    "                \"streaming\": True,\n",
    "                \"frame_count\": frame_count,\n",
    "                \"type\": \"initial\"\n",
    "            }\n",
    "            yield f\"data: {json.dumps(initial_response)}\\n\\n\"\n",
    "            \n",
    "            # Small delay before starting analysis\n",
    "            await asyncio.sleep(0.3)\n",
    "            \n",
    "            # Then stream mock GPT-like Two Sum solution\n",
    "            mock_gpt_responses = [\n",
    "                \"ğŸ¤– I can see you're working on the Two Sum problem! Let me help you solve this step by step.\\n\",\n",
    "                \"ğŸ“‹ **Problem Analysis:**\\nGiven an array of integers `nums` and an integer `target`, return indices of two numbers that add up to `target`.\\n\",\n",
    "                \"ğŸ’¡ **Approach 1: Brute Force**\\nWe could use nested loops to check every pair, but that would be O(nÂ²) time complexity.\\n\",\n",
    "                \"âš¡ **Better Approach: Hash Map**\\nLet's use a hash map to solve this in O(n) time!\\n\",\n",
    "                \"```python\\ndef twoSum(nums, target):\\n    num_map = {}\\n    for i, num in enumerate(nums):\\n        complement = target - num\\n        if complement in num_map:\\n            return [num_map[complement], i]\\n        num_map[num] = i\\n```\\n\",\n",
    "                \"ğŸ” **How it works:**\\n1. Create an empty hash map\\n2. For each number, calculate its complement (target - current number)\\n3. Check if complement exists in hash map\\n4. If yes, return the indices; if no, store current number and index\\n\",\n",
    "                \"ğŸ“Š **Time Complexity:** O(n) - single pass through array\\n**Space Complexity:** O(n) - hash map storage\\n\",\n",
    "                \"ğŸ§ª **Test with your example:**\\n`nums = [2,7,11,15], target = 9`\\n- i=0, num=2, complement=7, not in map, store {2:0}\\n- i=1, num=7, complement=2, found at index 0, return [0,1]\\n\",\n",
    "                \"âœ¨ **Alternative One-liner (Python):**\\n```python\\ndef twoSum(nums, target):\\n    seen = {}\\n    return next(([seen[target-n], i] for i, n in enumerate(nums) if target-n in seen or seen.setdefault(n, i)), None)\\n```\\n\",\n",
    "                \"ğŸ¯ **Key Insights:**\\n- Hash maps provide O(1) average lookup time\\n- We only need one pass through the array\\n- Always check if complement exists before storing current number\\n\",\n",
    "                \"ğŸ† **Solution Complete!** This approach efficiently solves Two Sum in linear time.\\n\"\n",
    "            ]\n",
    "            \n",
    "            for i, response in enumerate(mock_gpt_responses):\n",
    "                await asyncio.sleep(0.8)  # Slightly longer delay for reading\n",
    "                stream_data = {\n",
    "                    \"type\": \"stream\",\n",
    "                    \"content\": response,\n",
    "                    \"step\": i + 1,\n",
    "                    \"total_steps\": len(mock_gpt_responses)\n",
    "                }\n",
    "                yield f\"data: {json.dumps(stream_data)}\\n\\n\"\n",
    "            \n",
    "            # Final completion message with detected text\n",
    "            final_data = {\n",
    "                \"type\": \"complete\",\n",
    "                \"content\": \"ğŸ¯ Two Sum solution explained successfully!\\n\",\n",
    "                \"total_frames_processed\": frame_count,\n",
    "                \"detected_text\": f\"\"\"**Detected from {frame_count} frames:**\n",
    "\n",
    "**LeetCode Problem 1: Two Sum**\n",
    "\n",
    "**Problem Statement:**\n",
    "Given an array of integers nums and an integer target, return indices of the two numbers such that they add up to target.\n",
    "\n",
    "You may assume that each input would have exactly one solution, and you may not use the same element twice.\n",
    "\n",
    "**Example 1:**\n",
    "Input: nums = [2,7,11,15], target = 9\n",
    "Output: [0,1]\n",
    "Explanation: Because nums[0] + nums[1] == 9, we return [0, 1].\n",
    "\n",
    "**Example 2:**\n",
    "Input: nums = [3,2,4], target = 6\n",
    "Output: [1,2]\n",
    "\n",
    "**Constraints:**\n",
    "- 2 â‰¤ nums.length â‰¤ 10â´\n",
    "- -10â¹ â‰¤ nums[i] â‰¤ 10â¹\n",
    "- -10â¹ â‰¤ target â‰¤ 10â¹\n",
    "- Only one valid answer exists.\n",
    "\n",
    "**Follow-up:** Can you come up with an algorithm that is less than O(nÂ²) time complexity?\n",
    "\n",
    "**Code Editor shows:**\n",
    "```python\n",
    "class Solution:\n",
    "    def twoSum(self, nums: List[int], target: int) -> List[int]:\n",
    "        # Your solution here\n",
    "        pass\n",
    "```\"\"\"\n",
    "            }\n",
    "            yield f\"data: {json.dumps(final_data)}\\n\\n\"\n",
    "            \n",
    "            # Send the [DONE] signal that frontend is waiting for\n",
    "            yield \"data: [DONE]\\n\\n\"\n",
    "        \n",
    "        return StreamingResponse(\n",
    "            generate_stream(),\n",
    "            media_type=\"text/event-stream\",\n",
    "            headers={\n",
    "                \"Cache-Control\": \"no-cache\",\n",
    "                \"Connection\": \"keep-alive\",\n",
    "                \"Content-Type\": \"text/event-stream\",\n",
    "                \"Access-Control-Allow-Origin\": \"*\",\n",
    "                \"Access-Control-Allow-Methods\": \"*\",\n",
    "                \"Access-Control-Allow-Headers\": \"*\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return JSONResponse({\n",
    "            \"success\": False,\n",
    "            \"error\": f\"Processing failed: {str(e)}\"\n",
    "        })\n",
    "\n",
    "# Start server\n",
    "async def start_frame_saver_server():\n",
    "    import uvicorn\n",
    "    try:\n",
    "        print(\"ğŸš€ Starting FRAME READER & SAVER server on http://localhost:8000\")\n",
    "        print(\"ğŸ“ Frames will be saved to: frames/ directory\")\n",
    "        print(\"ğŸ“¡ Now includes streaming Two Sum solution!\")\n",
    "        \n",
    "        print(\"ğŸ’¡ Send your frontend request to save frames and get streaming output!\")\n",
    "        \n",
    "        config = uvicorn.Config(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
    "        server = uvicorn.Server(config)\n",
    "        await server.serve()\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Server error: {e}\")\n",
    "\n",
    "await start_frame_saver_server()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dfadb7",
   "metadata": {},
   "source": [
    "# ğŸ¤– AI-Powered LeetCode Assistant Server (Production Version)\n",
    "\n",
    "A **production-ready server** that connects to **OpenAI's GPT-4.1-mini** to analyze LeetCode problem screenshots and provide intelligent solutions in real-time.\n",
    "\n",
    "## ğŸ”‘ Prerequisites\n",
    "\n",
    "- OpenAI API key in `.env` file as `OPENAI_API_KEY`\n",
    "- Internet connection for API calls\n",
    "\n",
    "## ğŸ“‹ Endpoint Details\n",
    "\n",
    "**`POST /process-multiple-frames-stream`**\n",
    "\n",
    "**Input:** \n",
    "- Form data with image frames (`frame_0`, `frame_1`, etc.)\n",
    "- Frame count metadata\n",
    "\n",
    "**Output:** \n",
    "- Server-Sent Events stream with real AI analysis\n",
    "- Frames saved to local `frames/` directory  \n",
    "- Live streaming of problem solutions and code explanations\n",
    "\n",
    "## ğŸš€ Usage\n",
    "\n",
    "Set your OpenAI API key, run the server, and send POST request with LeetCode screenshot frames to get real AI-powered analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69c05005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Starting FRAME READER & OPENAI ANALYZER server on http://localhost:8000\n",
      "ğŸ“ Frames will be saved to: frames/ directory\n",
      "ğŸ¤– Now includes real OpenAI GPT-4.1-mini analysis!\n",
      "ğŸ”‘ Make sure your OPENAI_API_KEY is set in .env file\n",
      "ğŸ’¡ Send your frontend request to analyze LeetCode screenshots!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [21908]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "ğŸ”„ READING AND PROCESSING FRAMES...\n",
      "ğŸ“‹ Content-Type: multipart/form-data; boundary=----WebKitFormBoundary2ou7CtugfBxRbSjv\n",
      "==================================================\n",
      "ğŸ”„ READING AND PROCESSING FRAMES...\n",
      "ğŸ“‹ Content-Type: multipart/form-data; boundary=----WebKitFormBoundarytw2GiozeDSKkukvf\n",
      "ğŸ“¥ Form data items: 5\n",
      "ğŸ” Form keys: ['frame_0', 'frame_1', 'frame_2', 'frame_3', 'frame_count']\n",
      "ğŸ“ Processing frame_0: 365639 bytes\n",
      "âœ… Saved frame_0: frames/frame_20250709_153842_frame_0.png (720x480)\n",
      "ğŸ“ Processing frame_1: 398080 bytes\n",
      "âœ… Saved frame_1: frames/frame_20250709_153842_frame_1.png (720x480)\n",
      "ğŸ“ Processing frame_2: 418543 bytes\n",
      "âœ… Saved frame_2: frames/frame_20250709_153842_frame_2.png (720x480)\n",
      "ğŸ“ Processing frame_3: 337496 bytes\n",
      "âœ… Saved frame_3: frames/frame_20250709_153842_frame_3.png (720x480)\n",
      "ğŸ“Š Expected frame count: 4\n",
      "ğŸ‰ Successfully saved 4 frames to frames/ folder!\n",
      "ğŸ”„ Converting 4 images to base64 for OpenAI API...\n",
      "INFO:     127.0.0.1:50797 - \"POST /process-multiple-frames-stream HTTP/1.1\" 200 OK\n",
      "ğŸ“¥ Form data items: 5\n",
      "ğŸ” Form keys: ['frame_0', 'frame_1', 'frame_2', 'frame_3', 'frame_count']\n",
      "ğŸ“ Processing frame_0: 365639 bytes\n",
      "âœ… Saved frame_0: frames/frame_20250709_153842_frame_0.png (720x480)\n",
      "ğŸ“ Processing frame_1: 398080 bytes\n",
      "âœ… Saved frame_1: frames/frame_20250709_153842_frame_1.png (720x480)\n",
      "ğŸ“ Processing frame_2: 418543 bytes\n",
      "âœ… Saved frame_2: frames/frame_20250709_153842_frame_2.png (720x480)\n",
      "ğŸ“ Processing frame_3: 337496 bytes\n",
      "âœ… Saved frame_3: frames/frame_20250709_153842_frame_3.png (720x480)\n",
      "ğŸ“Š Expected frame count: 4\n",
      "ğŸ‰ Successfully saved 4 frames to frames/ folder!\n",
      "ğŸ”„ Converting 4 images to base64 for OpenAI API...\n",
      "INFO:     127.0.0.1:50798 - \"POST /process-multiple-frames-stream HTTP/1.1\" 200 OK\n",
      "ğŸ¤– Sending 4 images to OpenAI API...\n",
      "ğŸ¤– Sending 4 images to OpenAI API...\n",
      "âœ… OpenAI streaming completed!\n",
      "âœ… OpenAI streaming completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Shutting down\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [21908]\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from fastapi import FastAPI, Request\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi.responses import JSONResponse, StreamingResponse\n",
    "from PIL import Image\n",
    "import json\n",
    "import asyncio\n",
    "import io\n",
    "import os\n",
    "import base64\n",
    "from datetime import datetime\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Enable CORS\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "@app.post(\"/process-multiple-frames-stream\")\n",
    "async def simple_frame_reader_with_save(request: Request):\n",
    "    \"\"\"Endpoint that reads frames, saves them, converts to base64, and sends to OpenAI API with streaming response\"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(\"=\" * 50)\n",
    "        print(\"ğŸ”„ READING AND PROCESSING FRAMES...\")\n",
    "        \n",
    "        # Get content type\n",
    "        content_type = request.headers.get(\"content-type\", \"\")\n",
    "        print(f\"ğŸ“‹ Content-Type: {content_type}\")\n",
    "        \n",
    "        # Parse form data\n",
    "        form_data = await request.form()\n",
    "        print(f\"ğŸ“¥ Form data items: {len(form_data)}\")\n",
    "        print(f\"ğŸ” Form keys: {list(form_data.keys())}\")\n",
    "        \n",
    "        # Create frames directory\n",
    "        os.makedirs(\"frames\", exist_ok=True)\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        # Process frames and convert to base64\n",
    "        saved_frames = []\n",
    "        base64_images = []\n",
    "        frame_count = 0\n",
    "        \n",
    "        for key, value in form_data.items():\n",
    "            if key.startswith('frame_') and hasattr(value, 'read'):\n",
    "                try:\n",
    "                    # Read file data\n",
    "                    file_data = await value.read()\n",
    "                    print(f\"ğŸ“ Processing {key}: {len(file_data)} bytes\")\n",
    "                    \n",
    "                    # Open image\n",
    "                    img = Image.open(io.BytesIO(file_data))\n",
    "                    \n",
    "                    # Save frame with timestamp\n",
    "                    save_path = f\"frames/frame_{timestamp}_{key}.png\"\n",
    "                    img.save(save_path)\n",
    "                    \n",
    "                    # Convert to base64 for OpenAI API\n",
    "                    base64_img = base64.b64encode(file_data).decode('utf-8')\n",
    "                    base64_images.append(base64_img)\n",
    "                    \n",
    "                    saved_frames.append({\n",
    "                        \"key\": key,\n",
    "                        \"filename\": getattr(value, 'filename', 'unknown'),\n",
    "                        \"size_bytes\": len(file_data),\n",
    "                        \"image_size\": f\"{img.width}x{img.height}\",\n",
    "                        \"saved_path\": save_path\n",
    "                    })\n",
    "                    \n",
    "                    frame_count += 1\n",
    "                    print(f\"âœ… Saved {key}: {save_path} ({img.width}x{img.height})\")\n",
    "                    \n",
    "                except Exception as frame_error:\n",
    "                    print(f\"âŒ Error processing {key}: {frame_error}\")\n",
    "            \n",
    "            elif key == 'frame_count':\n",
    "                expected_count = str(value)\n",
    "                print(f\"ğŸ“Š Expected frame count: {expected_count}\")\n",
    "        \n",
    "        print(f\"ğŸ‰ Successfully saved {frame_count} frames to frames/ folder!\")\n",
    "        print(f\"ğŸ”„ Converting {len(base64_images)} images to base64 for OpenAI API...\")\n",
    "        \n",
    "        # Now create streaming response with initial success data + real OpenAI streaming\n",
    "        async def generate_stream():\n",
    "            # First yield the success response\n",
    "            initial_response = {\n",
    "                \"success\": True,\n",
    "                \"message\": f\"Successfully received and saved {frame_count} frames!\",\n",
    "                \"frames_saved\": saved_frames,\n",
    "                \"timestamp\": timestamp,\n",
    "                \"save_directory\": \"frames/\",\n",
    "                \"streaming\": True,\n",
    "                \"frame_count\": frame_count,\n",
    "                \"type\": \"initial\"\n",
    "            }\n",
    "            yield f\"data: {json.dumps(initial_response)}\\n\\n\"\n",
    "            \n",
    "            # Small delay before starting analysis\n",
    "            await asyncio.sleep(0.3)\n",
    "            \n",
    "            try:\n",
    "                # Prepare content for OpenAI API\n",
    "                content = [\n",
    "                    {\"type\": \"text\", \"text\": \"Analyze these LeetCode problem screenshots. Explain the problem and provide a detailed solution with code. If multiple frames show the same problem, focus on the clearest view. If frames show different problems, analyze each one.\"}\n",
    "                ]\n",
    "                \n",
    "                # Add all base64 images to the content\n",
    "                for base64_img in base64_images:\n",
    "                    content.append({\n",
    "                        \"type\": \"image_url\", \n",
    "                        \"image_url\": {\"url\": f\"data:image/png;base64,{base64_img}\"}\n",
    "                    })\n",
    "                \n",
    "                print(f\"ğŸ¤– Sending {len(base64_images)} images to OpenAI API...\")\n",
    "                \n",
    "                # Stream response from OpenAI\n",
    "                stream = client.chat.completions.create(\n",
    "                    model=\"gpt-4.1-mini\",\n",
    "                    messages=[\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": content\n",
    "                        }\n",
    "                    ],\n",
    "                    temperature=0.2,\n",
    "                    stream=True\n",
    "                )\n",
    "                \n",
    "                accumulated_content = \"\"\n",
    "                step = 0\n",
    "                \n",
    "                for chunk in stream:\n",
    "                    # Handle content chunks\n",
    "                    if chunk.choices and chunk.choices[0].delta.content is not None:\n",
    "                        content_chunk = chunk.choices[0].delta.content\n",
    "                        accumulated_content += content_chunk\n",
    "                        step += 1\n",
    "                        \n",
    "                        stream_data = {\n",
    "                            \"type\": \"stream\",\n",
    "                            \"content\": content_chunk,\n",
    "                            \"step\": step,\n",
    "                            \"accumulated\": accumulated_content\n",
    "                        }\n",
    "                        yield f\"data: {json.dumps(stream_data)}\\n\\n\"\n",
    "                        \n",
    "                        # Small delay to make streaming visible\n",
    "                        await asyncio.sleep(0.01)\n",
    "                \n",
    "                print(\"âœ… OpenAI streaming completed!\")\n",
    "                \n",
    "            except Exception as api_error:\n",
    "                print(f\"âŒ OpenAI API Error: {api_error}\")\n",
    "                error_data = {\n",
    "                    \"type\": \"stream\",\n",
    "                    \"content\": f\"âŒ Error calling OpenAI API: {str(api_error)}\\n\\nUsing fallback response...\\n\",\n",
    "                    \"error\": True\n",
    "                }\n",
    "                yield f\"data: {json.dumps(error_data)}\\n\\n\"\n",
    "                \n",
    "                # Fallback message\n",
    "                fallback_data = {\n",
    "                    \"type\": \"stream\",\n",
    "                    \"content\": \"ğŸ¤– Unable to analyze images with AI. Please check your OpenAI API key and try again.\\n\"\n",
    "                }\n",
    "                yield f\"data: {json.dumps(fallback_data)}\\n\\n\"\n",
    "            \n",
    "            # Final completion message\n",
    "            final_data = {\n",
    "                \"type\": \"complete\",\n",
    "                \"content\": \"ğŸ¯ Analysis complete!\\n\",\n",
    "                \"total_frames_processed\": frame_count,\n",
    "                \"detected_text\": f\"\"\"**Placeholder for detected text from {frame_count} frames:**\n",
    "\n",
    "This will be replaced with actual OCR text extraction in future versions.\n",
    "For now, the AI analysis above contains the problem understanding and solution.\n",
    "\n",
    "**Technical Details:**\n",
    "- Frames processed: {frame_count}\n",
    "- Images sent to AI: {len(base64_images)}\n",
    "- Timestamp: {timestamp}\n",
    "- Save location: frames/\n",
    "\n",
    "**Next Steps:**\n",
    "- Implement OCR text extraction\n",
    "- Add text preprocessing\n",
    "- Enhance problem detection accuracy\"\"\"\n",
    "            }\n",
    "            yield f\"data: {json.dumps(final_data)}\\n\\n\"\n",
    "            \n",
    "            # Send the [DONE] signal that frontend is waiting for\n",
    "            yield \"data: [DONE]\\n\\n\"\n",
    "        \n",
    "        return StreamingResponse(\n",
    "            generate_stream(),\n",
    "            media_type=\"text/event-stream\",\n",
    "            headers={\n",
    "                \"Cache-Control\": \"no-cache\",\n",
    "                \"Connection\": \"keep-alive\",\n",
    "                \"Content-Type\": \"text/event-stream\",\n",
    "                \"Access-Control-Allow-Origin\": \"*\",\n",
    "                \"Access-Control-Allow-Methods\": \"*\",\n",
    "                \"Access-Control-Allow-Headers\": \"*\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return JSONResponse({\n",
    "            \"success\": False,\n",
    "            \"error\": f\"Processing failed: {str(e)}\"\n",
    "        })\n",
    "\n",
    "# Start server\n",
    "async def start_frame_saver_server():\n",
    "    import uvicorn\n",
    "    try:\n",
    "        print(\"ğŸš€ Starting FRAME READER & OPENAI ANALYZER server on http://localhost:8000\")\n",
    "        print(\"ğŸ“ Frames will be saved to: frames/ directory\")\n",
    "        print(\"ğŸ¤– Now includes real OpenAI GPT-4.1-mini analysis!\")\n",
    "        print(\"ğŸ”‘ Make sure your OPENAI_API_KEY is set in .env file\")\n",
    "        \n",
    "        print(\"ğŸ’¡ Send your frontend request to analyze LeetCode screenshots!\")\n",
    "        \n",
    "        config = uvicorn.Config(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
    "        server = uvicorn.Server(config)\n",
    "        await server.serve()\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Server error: {e}\")\n",
    "\n",
    "await start_frame_saver_server()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MIT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
