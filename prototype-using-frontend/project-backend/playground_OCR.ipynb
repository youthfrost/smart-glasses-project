{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24211645",
   "metadata": {},
   "source": [
    "# 🧪 Mock LeetCode Assistant Server (Testing Version)\n",
    "\n",
    "A **hardcoded testing server** that simulates LeetCode problem analysis without connecting to OpenAI's API. Returns a pre-written Two Sum solution for development and testing purposes.\n",
    "\n",
    "## 📋 Endpoint Details\n",
    "\n",
    "**`POST /process-multiple-frames-stream`**\n",
    "\n",
    "**Input:** \n",
    "- Form data with image frames (`frame_0`, `frame_1`, etc.)\n",
    "- Frame count metadata\n",
    "\n",
    "**Output:** \n",
    "- Server-Sent Events stream with mock Two Sum solution\n",
    "- Frames saved to local `frames/` directory\n",
    "- Mock detected text and problem explanation\n",
    "\n",
    "## 🚀 Usage\n",
    "\n",
    "Run the server and send POST request with image frames to get streaming mock AI response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52506a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fastapi\n",
      "  Downloading fastapi-0.116.0-py3-none-any.whl (95 kB)\n",
      "                                              0.0/95.6 kB ? eta -:--:--\n",
      "                                              0.0/95.6 kB ? eta -:--:--\n",
      "                                              0.0/95.6 kB ? eta -:--:--\n",
      "                                              0.0/95.6 kB ? eta -:--:--\n",
      "                                              0.0/95.6 kB ? eta -:--:--\n",
      "     ----                                     10.2/95.6 kB ? eta -:--:--\n",
      "     ----                                     10.2/95.6 kB ? eta -:--:--\n",
      "     ------------                           30.7/95.6 kB 220.2 kB/s eta 0:00:01\n",
      "     ------------                           30.7/95.6 kB 220.2 kB/s eta 0:00:01\n",
      "     ------------                           30.7/95.6 kB 220.2 kB/s eta 0:00:01\n",
      "     ----------------                       41.0/95.6 kB 151.3 kB/s eta 0:00:01\n",
      "     ------------------------               61.4/95.6 kB 204.8 kB/s eta 0:00:01\n",
      "     ------------------------               61.4/95.6 kB 204.8 kB/s eta 0:00:01\n",
      "     ------------------------------------   92.2/95.6 kB 210.1 kB/s eta 0:00:01\n",
      "     -------------------------------------- 95.6/95.6 kB 210.3 kB/s eta 0:00:00\n",
      "Collecting uvicorn\n",
      "  Downloading uvicorn-0.35.0-py3-none-any.whl (66 kB)\n",
      "                                              0.0/66.4 kB ? eta -:--:--\n",
      "     ------                                   10.2/66.4 kB ? eta -:--:--\n",
      "     -----------------------                41.0/66.4 kB 991.0 kB/s eta 0:00:01\n",
      "     -------------------------------------- 66.4/66.4 kB 720.1 kB/s eta 0:00:00\n",
      "Collecting starlette<0.47.0,>=0.40.0 (from fastapi)\n",
      "  Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
      "                                              0.0/72.0 kB ? eta -:--:--\n",
      "     -----------------                        30.7/72.0 kB 1.3 MB/s eta 0:00:01\n",
      "     ---------------------                  41.0/72.0 kB 653.6 kB/s eta 0:00:01\n",
      "     -------------------------------------- 72.0/72.0 kB 657.4 kB/s eta 0:00:00\n",
      "Collecting pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 (from fastapi)\n",
      "  Downloading pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "                                              0.0/444.8 kB ? eta -:--:--\n",
      "                                              10.2/444.8 kB ? eta -:--:--\n",
      "     -----                                 61.4/444.8 kB 825.8 kB/s eta 0:00:01\n",
      "     -----                                 71.7/444.8 kB 653.6 kB/s eta 0:00:01\n",
      "     --------                             102.4/444.8 kB 658.3 kB/s eta 0:00:01\n",
      "     -----------                          143.4/444.8 kB 655.8 kB/s eta 0:00:01\n",
      "     --------------                       174.1/444.8 kB 655.4 kB/s eta 0:00:01\n",
      "     --------------                       184.3/444.8 kB 619.5 kB/s eta 0:00:01\n",
      "     ------------------                   225.3/444.8 kB 655.6 kB/s eta 0:00:01\n",
      "     --------------------                 256.0/444.8 kB 655.4 kB/s eta 0:00:01\n",
      "     ------------------------             307.2/444.8 kB 703.7 kB/s eta 0:00:01\n",
      "     -------------------------------      389.1/444.8 kB 807.8 kB/s eta 0:00:01\n",
      "     ------------------------------------ 444.8/444.8 kB 843.8 kB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\obscu\\appdata\\roaming\\python\\python311\\site-packages (from fastapi) (4.14.1)\n",
      "Collecting click>=7.0 (from uvicorn)\n",
      "  Downloading click-8.2.1-py3-none-any.whl (102 kB)\n",
      "                                              0.0/102.2 kB ? eta -:--:--\n",
      "     -----------------------------------     92.2/102.2 kB 2.6 MB/s eta 0:00:01\n",
      "     -------------------------------------- 102.2/102.2 kB 2.0 MB/s eta 0:00:00\n",
      "Collecting h11>=0.8 (from uvicorn)\n",
      "  Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\obscu\\appdata\\roaming\\python\\python311\\site-packages (from click>=7.0->uvicorn) (0.4.6)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi)\n",
      "  Downloading pydantic_core-2.33.2-cp311-cp311-win_amd64.whl (2.0 MB)\n",
      "                                              0.0/2.0 MB ? eta -:--:--\n",
      "     -                                        0.1/2.0 MB 1.6 MB/s eta 0:00:02\n",
      "     ---------                                0.5/2.0 MB 5.0 MB/s eta 0:00:01\n",
      "     ----------                               0.5/2.0 MB 4.5 MB/s eta 0:00:01\n",
      "     ----------                               0.5/2.0 MB 4.5 MB/s eta 0:00:01\n",
      "     -----------                              0.6/2.0 MB 2.4 MB/s eta 0:00:01\n",
      "     --------------                           0.7/2.0 MB 2.5 MB/s eta 0:00:01\n",
      "     ------------------                       0.9/2.0 MB 2.7 MB/s eta 0:00:01\n",
      "     ---------------------                    1.0/2.0 MB 2.8 MB/s eta 0:00:01\n",
      "     ------------------------                 1.2/2.0 MB 2.9 MB/s eta 0:00:01\n",
      "     ----------------------------             1.4/2.0 MB 3.0 MB/s eta 0:00:01\n",
      "     ---------------------------------        1.6/2.0 MB 3.1 MB/s eta 0:00:01\n",
      "     --------------------------------------   1.9/2.0 MB 3.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.0/2.0 MB 3.4 MB/s eta 0:00:00\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi)\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Collecting anyio<5,>=3.6.2 (from starlette<0.47.0,>=0.40.0->fastapi)\n",
      "  Downloading anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "                                              0.0/100.9 kB ? eta -:--:--\n",
      "     -------------------------------------- 100.9/100.9 kB 6.0 MB/s eta 0:00:00\n",
      "Collecting idna>=2.8 (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi)\n",
      "  Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "                                              0.0/70.4 kB ? eta -:--:--\n",
      "     ---------------------------------------- 70.4/70.4 kB 4.0 MB/s eta 0:00:00\n",
      "Collecting sniffio>=1.1 (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi)\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: typing-inspection, sniffio, pydantic-core, idna, h11, click, annotated-types, uvicorn, pydantic, anyio, starlette, fastapi\n",
      "Successfully installed annotated-types-0.7.0 anyio-4.9.0 click-8.2.1 fastapi-0.116.0 h11-0.16.0 idna-3.10 pydantic-2.11.7 pydantic-core-2.33.2 sniffio-1.3.1 starlette-0.46.2 typing-inspection-0.4.1 uvicorn-0.35.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script uvicorn.exe is installed in 'c:\\Users\\obscu\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script fastapi.exe is installed in 'c:\\Users\\obscu\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install fastapi uvicorn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dfadb7",
   "metadata": {},
   "source": [
    "# 🤖 AI-Powered LeetCode Assistant Server (Production Version)\n",
    "\n",
    "A **production-ready server** that connects to **OpenAI's GPT-4.1-mini** to analyze LeetCode problem screenshots and provide intelligent solutions in real-time.\n",
    "\n",
    "## 🔑 Prerequisites\n",
    "\n",
    "- OpenAI API key in `.env` file as `OPENAI_API_KEY`\n",
    "- Internet connection for API calls\n",
    "\n",
    "## 📋 Endpoint Details\n",
    "\n",
    "**`POST /process-multiple-frames-stream`**\n",
    "\n",
    "**Input:** \n",
    "- Form data with image frames (`frame_0`, `frame_1`, etc.)\n",
    "- Frame count metadata\n",
    "\n",
    "**Output:** \n",
    "- Server-Sent Events stream with real AI analysis\n",
    "- Frames saved to local `frames/` directory  \n",
    "- Live streaming of problem solutions and code explanations\n",
    "\n",
    "## 🚀 Usage\n",
    "\n",
    "Set your OpenAI API key, run the server, and send POST request with LeetCode screenshot frames to get real AI-powered analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ca1b761c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dotenv in c:\\users\\obscu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.9.9)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\obscu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from dotenv) (1.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2da21756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\obscu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.28.0)\n",
      "Collecting openai\n",
      "  Downloading openai-1.97.0-py3-none-any.whl (764 kB)\n",
      "                                              0.0/765.0 kB ? eta -:--:--\n",
      "     --------                               174.1/765.0 kB 5.1 MB/s eta 0:00:01\n",
      "     ---------------------------------      665.6/765.0 kB 8.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- 765.0/765.0 kB 6.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\obscu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\obscu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\obscu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\obscu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\obscu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (2.11.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\obscu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\obscu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\obscu\\appdata\\roaming\\python\\python311\\site-packages (from openai) (4.14.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\obscu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\obscu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.7.9)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\obscu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\obscu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\obscu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\obscu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\obscu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\obscu\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Installing collected packages: openai\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 0.28.0\n",
      "    Uninstalling openai-0.28.0:\n",
      "      Successfully uninstalled openai-0.28.0\n",
      "Successfully installed openai-1.97.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script openai.exe is installed in 'c:\\Users\\obscu\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install openai --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f68122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2. Add Two Numbers\\n\\n9 Topics\\n\\nYou are given two non-empty linked lists representing two non-negative integers. The digits are stored in\\nreverse order, and each of their nodes contains a single digit. Add the two numbers and return the sum as\\na linked list.\\n\\nYou may assume the two numbers do not contain any leading zero, except the number 0 itself.\\n\\nExample 1:\\n\\nInput: 11 = [2,4,3], 12 = [5,6,4]'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install tesseract, testing cell\n",
    "\n",
    "import cv2\n",
    "import pytesseract\n",
    "import os\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = 'C:\\Program Files\\Tesseract-OCR\\Tesseract.exe'  # Update this path if necessary\n",
    "\n",
    "\n",
    "def OCR_from_image(image_path):\n",
    "    \"\"\"Extract text from an image using Tesseract OCR.\"\"\"\n",
    "    try:\n",
    "        # Read the image using OpenCV\n",
    "        img = cv2.imread(image_path)\n",
    "        if img is None:\n",
    "            raise ValueError(f\"Could not read image at {image_path}\")\n",
    "        # Convert the image to RGB (Tesseract expects RGB format)\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        # Use Tesseract to do OCR on the image\n",
    "        text = pytesseract.image_to_string(img_rgb)\n",
    "        return text.strip()  # Return the extracted text, stripping any extra whitespace\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during OCR processing: {e}\")\n",
    "        return None\n",
    "    \n",
    "# Example usage\n",
    "image = \"img.jpg\"  # Path to your image file\n",
    "OCR_from_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951d2843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting FRAME READER & OPENAI ANALYZER server on http://localhost:8000\n",
      "📁 Frames will be saved to: frames/ directory\n",
      "🤖 Now includes real OpenAI GPT-4.1-mini analysis!\n",
      "🔑 Make sure your OPENAI_API_KEY is set in .env file\n",
      "💡 Send your frontend request to analyze LeetCode screenshots!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [36052]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "🔄 READING AND PROCESSING FRAMES...\n",
      "📋 Content-Type: multipart/form-data; boundary=----WebKitFormBoundaryi1H8o4hI5QzflqIN\n",
      "==================================================\n",
      "🔄 READING AND PROCESSING FRAMES...\n",
      "📋 Content-Type: multipart/form-data; boundary=----WebKitFormBoundaryH2iaCtQmUAO2IiYL\n",
      "📥 Form data items: 5\n",
      "🔍 Form keys: ['frame_0', 'frame_1', 'frame_2', 'frame_3', 'frame_count']\n",
      "📥 Form data items: 5\n",
      "🔍 Form keys: ['frame_0', 'frame_1', 'frame_2', 'frame_3', 'frame_count']\n",
      "📝 Processing frame_0: 1335238 bytes\n",
      "✅ Saved frame_0: frames/frame_20250718_234134_frame_0.png (1280x720)\n",
      "📝 Processing frame_0: 1335238 bytes\n",
      "✅ Saved frame_0: frames/frame_20250718_234134_frame_0.png (1280x720)\n",
      "📝 Processing frame_1: 1232735 bytes\n",
      "✅ Saved frame_1: frames/frame_20250718_234134_frame_1.png (1280x720)\n",
      "📝 Processing frame_1: 1232735 bytes\n",
      "✅ Saved frame_1: frames/frame_20250718_234134_frame_1.png (1280x720)\n",
      "📝 Processing frame_2: 1360445 bytes\n",
      "✅ Saved frame_2: frames/frame_20250718_234134_frame_2.png (1280x720)\n",
      "📝 Processing frame_2: 1360445 bytes\n",
      "✅ Saved frame_2: frames/frame_20250718_234134_frame_2.png (1280x720)\n",
      "📝 Processing frame_3: 1422514 bytes\n",
      "✅ Saved frame_3: frames/frame_20250718_234134_frame_3.png (1280x720)\n",
      "📊 Expected frame count: 4\n",
      "🎉 Successfully saved 4 frames to frames/ folder!\n",
      "🔄 Converting 4 images to base64 for OpenAI API...\n",
      "INFO:     127.0.0.1:52133 - \"POST /process-multiple-frames-stream HTTP/1.1\" 200 OK\n",
      "📝 Processing frame_3: 1422514 bytes\n",
      "✅ Saved frame_3: frames/frame_20250718_234134_frame_3.png (1280x720)\n",
      "📊 Expected frame count: 4\n",
      "🎉 Successfully saved 4 frames to frames/ folder!\n",
      "🔄 Converting 4 images to base64 for OpenAI API...\n",
      "INFO:     127.0.0.1:52132 - \"POST /process-multiple-frames-stream HTTP/1.1\" 200 OK\n",
      "🔤 Extracted text from 4 images using OCR.\n",
      "🔤 Extracted text from 4 images using OCR.\n",
      "✅ OpenAI streaming completed!\n",
      "✅ OpenAI streaming completed!\n",
      "==================================================\n",
      "🔄 READING AND PROCESSING FRAMES...\n",
      "📋 Content-Type: multipart/form-data; boundary=----WebKitFormBoundaryJPUAiei7h79d0xlh\n",
      "==================================================\n",
      "🔄 READING AND PROCESSING FRAMES...\n",
      "📋 Content-Type: multipart/form-data; boundary=----WebKitFormBoundaryVaBoVZScfTgkBRNN\n",
      "📥 Form data items: 5\n",
      "🔍 Form keys: ['frame_0', 'frame_1', 'frame_2', 'frame_3', 'frame_count']\n",
      "📥 Form data items: 5\n",
      "🔍 Form keys: ['frame_0', 'frame_1', 'frame_2', 'frame_3', 'frame_count']\n",
      "📝 Processing frame_0: 1301742 bytes\n",
      "✅ Saved frame_0: frames/frame_20250718_234339_frame_0.png (1280x720)\n",
      "📝 Processing frame_0: 1301742 bytes\n",
      "✅ Saved frame_0: frames/frame_20250718_234339_frame_0.png (1280x720)\n",
      "📝 Processing frame_1: 1330322 bytes\n",
      "✅ Saved frame_1: frames/frame_20250718_234339_frame_1.png (1280x720)\n",
      "📝 Processing frame_1: 1330322 bytes\n",
      "✅ Saved frame_1: frames/frame_20250718_234339_frame_1.png (1280x720)\n",
      "📝 Processing frame_2: 1274660 bytes\n",
      "✅ Saved frame_2: frames/frame_20250718_234339_frame_2.png (1280x720)\n",
      "📝 Processing frame_2: 1274660 bytes\n",
      "✅ Saved frame_2: frames/frame_20250718_234339_frame_2.png (1280x720)\n",
      "📝 Processing frame_3: 1353162 bytes\n",
      "✅ Saved frame_3: frames/frame_20250718_234339_frame_3.png (1280x720)\n",
      "📊 Expected frame count: 4\n",
      "🎉 Successfully saved 4 frames to frames/ folder!\n",
      "🔄 Converting 4 images to base64 for OpenAI API...\n",
      "📝 Processing frame_3: 1353162 bytes\n",
      "✅ Saved frame_3: frames/frame_20250718_234339_frame_3.png (1280x720)\n",
      "📊 Expected frame count: 4\n",
      "🎉 Successfully saved 4 frames to frames/ folder!\n",
      "🔄 Converting 4 images to base64 for OpenAI API...\n",
      "INFO:     127.0.0.1:52357 - \"POST /process-multiple-frames-stream HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52358 - \"POST /process-multiple-frames-stream HTTP/1.1\" 200 OK\n",
      "🔤 Extracted text from 4 images using OCR.\n",
      "🔤 Extracted text from 4 images using OCR.\n",
      "✅ OpenAI streaming completed!\n",
      "✅ OpenAI streaming completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Shutting down\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [36052]\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from fastapi import FastAPI, Request\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi.responses import JSONResponse, StreamingResponse\n",
    "from PIL import Image\n",
    "import json\n",
    "import asyncio\n",
    "import io\n",
    "import os\n",
    "import base64\n",
    "from datetime import datetime\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import pytesseract\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))  # Replace with your OpenAI API key\n",
    "\n",
    "app = FastAPI()\n",
    "pytesseract.pytesseract.tesseract_cmd = 'C:\\Program Files\\Tesseract-OCR\\Tesseract.exe'  # Update this path if necessary\n",
    "\n",
    "# Enable CORS\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "@app.post(\"/process-multiple-frames-stream\")\n",
    "async def simple_frame_reader_with_save(request: Request):\n",
    "    \"\"\"Endpoint that reads frames, saves them, converts to base64, and sends to OpenAI API with streaming response\"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(\"=\" * 50)\n",
    "        print(\"🔄 READING AND PROCESSING FRAMES...\")\n",
    "        \n",
    "        # Get content type\n",
    "        content_type = request.headers.get(\"content-type\", \"\")\n",
    "        print(f\"📋 Content-Type: {content_type}\")\n",
    "        \n",
    "        # Parse form data\n",
    "        form_data = await request.form()\n",
    "        print(f\"📥 Form data items: {len(form_data)}\")\n",
    "        print(f\"🔍 Form keys: {list(form_data.keys())}\")\n",
    "        \n",
    "        # Create frames directory\n",
    "        os.makedirs(\"frames\", exist_ok=True)\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        # Process frames and convert to base64\n",
    "        saved_frames = []\n",
    "        base64_images = []\n",
    "        frame_count = 0\n",
    "        \n",
    "        for key, value in form_data.items():\n",
    "            if key.startswith('frame_') and hasattr(value, 'read'):\n",
    "                try:\n",
    "                    # Read file data\n",
    "                    file_data = await value.read()\n",
    "                    print(f\"📝 Processing {key}: {len(file_data)} bytes\")\n",
    "                    \n",
    "                    # Open image\n",
    "                    img = Image.open(io.BytesIO(file_data))\n",
    "                    \n",
    "                    # Save frame with timestamp\n",
    "                    save_path = f\"frames/frame_{timestamp}_{key}.png\"\n",
    "                    img.save(save_path)\n",
    "                    \n",
    "                    # Convert to base64 for OpenAI API\n",
    "                    base64_img = base64.b64encode(file_data).decode('utf-8')\n",
    "                    base64_images.append(base64_img)\n",
    "                    \n",
    "                    saved_frames.append({\n",
    "                        \"key\": key,\n",
    "                        \"filename\": getattr(value, 'filename', 'unknown'),\n",
    "                        \"size_bytes\": len(file_data),\n",
    "                        \"image_size\": f\"{img.width}x{img.height}\",\n",
    "                        \"saved_path\": save_path\n",
    "                    })\n",
    "                    \n",
    "                    frame_count += 1\n",
    "                    print(f\"✅ Saved {key}: {save_path} ({img.width}x{img.height})\")\n",
    "                    \n",
    "                except Exception as frame_error:\n",
    "                    print(f\"❌ Error processing {key}: {frame_error}\")\n",
    "            \n",
    "            elif key == 'frame_count':\n",
    "                expected_count = str(value)\n",
    "                print(f\"📊 Expected frame count: {expected_count}\")\n",
    "        \n",
    "        print(f\"🎉 Successfully saved {frame_count} frames to frames/ folder!\")\n",
    "        print(f\"🔄 Converting {len(base64_images)} images to base64 for OpenAI API...\")\n",
    "        \n",
    "        # Now create streaming response with initial success data + real OpenAI streaming\n",
    "        async def generate_stream():\n",
    "            # First yield the success response\n",
    "            initial_response = {\n",
    "                \"success\": True,\n",
    "                \"message\": f\"Successfully received and saved {frame_count} frames!\",\n",
    "                \"frames_saved\": saved_frames,\n",
    "                \"timestamp\": timestamp,\n",
    "                \"save_directory\": \"frames/\",\n",
    "                \"streaming\": True,\n",
    "                \"frame_count\": frame_count,\n",
    "                \"type\": \"initial\"\n",
    "            }\n",
    "            yield f\"data: {json.dumps(initial_response)}\\n\\n\"\n",
    "            \n",
    "            # Small delay before starting analysis\n",
    "            await asyncio.sleep(0.3)\n",
    "            \n",
    "            try:\n",
    "                # Prepare content for OpenAI API\n",
    "                content = [\n",
    "                    {\"type\": \"text\", \"text\": \"Analyze these LeetCode problem. Explain the problem and provide a detailed solution with code. If different problems are shown, analyze each one.\"}\n",
    "                ]\n",
    "                Time_before_OCR = time.time()\n",
    "\n",
    "                # Perform OCR on each image and add text content *(new feature by tesseract)*\n",
    "                ocr_texts = []\n",
    "                for base64_img in base64_images:\n",
    "                    # Decode base64 image\n",
    "                    img_data = base64.b64decode(base64_img)\n",
    "                    img = Image.open(io.BytesIO(img_data))\n",
    "                    \n",
    "                    # Perform OCR\n",
    "                    ocr_text = pytesseract.image_to_string(img)\n",
    "                    ocr_texts.append(ocr_text.strip())\n",
    "                    \n",
    "                    # Add OCR text to content\n",
    "                    content.append({\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": f\"Extracted text from image: {ocr_text}\"\n",
    "                    })\n",
    "                print(f\"🔤 Extracted text from {len(ocr_texts)} images using OCR.\")\n",
    "\n",
    "                # Add all base64 images to the content*(old feature, sending img to OpenAI)*\n",
    "                #for base64_img in base64_images:\n",
    "                    #content.append({\n",
    "                        #\"type\": \"image_url\", \n",
    "                        #\"image_url\": {\"url\": f\"data:image/png;base64,{base64_img}\"}\n",
    "                    #})\n",
    "                \n",
    "               #print(f\"🤖 Sending {len(base64_images)} images to OpenAI API...\")\n",
    "\n",
    "                \n",
    "\n",
    "                # Stream response from OpenAI\n",
    "                stream = client.chat.completions.create(\n",
    "                    model=\"gpt-4.1-mini\",\n",
    "                    messages=[\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": content\n",
    "                        }\n",
    "                    ],\n",
    "                    temperature=0.2,\n",
    "                    stream=True\n",
    "                )\n",
    "                \n",
    "                accumulated_content = \"\"\n",
    "                step = 0\n",
    "                \n",
    "                for chunk in stream:\n",
    "                    # Handle content chunks\n",
    "                    if chunk.choices and chunk.choices[0].delta.content is not None:\n",
    "                        content_chunk = chunk.choices[0].delta.content\n",
    "                        accumulated_content += content_chunk\n",
    "                        step += 1\n",
    "                        \n",
    "                        stream_data = {\n",
    "                            \"type\": \"stream\",\n",
    "                            \"content\": content_chunk,\n",
    "                            \"step\": step,\n",
    "                            \"accumulated\": accumulated_content\n",
    "                        }\n",
    "                        yield f\"data: {json.dumps(stream_data)}\\n\\n\"\n",
    "                        \n",
    "                        # Small delay to make streaming visible\n",
    "                        await asyncio.sleep(0.01)\n",
    "                \n",
    "                print(\"✅ OpenAI streaming completed!\")\n",
    "                Time_after_processing = time.time()\n",
    "                Time_taken = Time_after_processing - Time_before_OCR\n",
    "                \n",
    "            except Exception as api_error:\n",
    "                print(f\"❌ OpenAI API Error: {api_error}\")\n",
    "                error_data = {\n",
    "                    \"type\": \"stream\",\n",
    "                    \"content\": f\"❌ Error calling OpenAI API: {str(api_error)}\\n\\nUsing fallback response...\\n\",\n",
    "                    \"error\": True\n",
    "                }\n",
    "                yield f\"data: {json.dumps(error_data)}\\n\\n\"\n",
    "                \n",
    "                # Fallback message\n",
    "                fallback_data = {\n",
    "                    \"type\": \"stream\",\n",
    "                    \"content\": \"🤖 Unable to analyze images with AI. Please check your OpenAI API key and try again.\\n\"\n",
    "                }\n",
    "                yield f\"data: {json.dumps(fallback_data)}\\n\\n\"\n",
    "            \n",
    "            # Final completion message\n",
    "            final_data = {\n",
    "                \"type\": \"complete\",\n",
    "                \"content\": \"🎯 Analysis complete!\\n\",\n",
    "                \"total_frames_processed\": frame_count,\n",
    "                \"detected_text\": f\"\"\"**Placeholder for detected text from {frame_count} frames:**\n",
    "\n",
    "This will be replaced with actual OCR text extraction in future versions.\n",
    "For now, the AI analysis above contains the problem understanding and solution.\n",
    "\n",
    "**Technical Details:**\n",
    "- Frames processed: {frame_count}\n",
    "- Images sent to AI: {len(base64_images)}\n",
    "- Timestamp: {timestamp}\n",
    "- Save location: frames/\n",
    "- OCR Text Extracted: {len(ocr_texts)} frames\n",
    "- Time taken: {Time_taken:.2f} seconds\n",
    "\n",
    "***Warning: Redirect the path to tesseract.exe for OCR if needed. ***\n",
    "\n",
    "**Next Steps:**\n",
    "- Implement OCR text extraction (prototype made by pytesseract)\n",
    "- Add text preprocessing\n",
    "- Enhance problem detection accuracy\"\"\"\n",
    "            }\n",
    "            yield f\"data: {json.dumps(final_data)}\\n\\n\"\n",
    "            \n",
    "            # Send the [DONE] signal that frontend is waiting for\n",
    "            yield \"data: [DONE]\\n\\n\"\n",
    "        \n",
    "        return StreamingResponse(\n",
    "            generate_stream(),\n",
    "            media_type=\"text/event-stream\",\n",
    "            headers={\n",
    "                \"Cache-Control\": \"no-cache\",\n",
    "                \"Connection\": \"keep-alive\",\n",
    "                \"Content-Type\": \"text/event-stream\",\n",
    "                \"Access-Control-Allow-Origin\": \"*\",\n",
    "                \"Access-Control-Allow-Methods\": \"*\",\n",
    "                \"Access-Control-Allow-Headers\": \"*\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return JSONResponse({\n",
    "            \"success\": False,\n",
    "            \"error\": f\"Processing failed: {str(e)}\"\n",
    "        })\n",
    "\n",
    "# Start server\n",
    "async def start_frame_saver_server():\n",
    "    import uvicorn\n",
    "    try:\n",
    "        print(\"🚀 Starting FRAME READER & OPENAI ANALYZER server on http://localhost:8000\")\n",
    "        print(\"📁 Frames will be saved to: frames/ directory\")\n",
    "        print(\"🤖 Now includes real OpenAI GPT-4.1-mini analysis!\")\n",
    "        print(\"🔑 Make sure your OPENAI_API_KEY is set in .env file\")\n",
    "        \n",
    "        print(\"💡 Send your frontend request to analyze LeetCode screenshots!\")\n",
    "        \n",
    "        config = uvicorn.Config(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
    "        server = uvicorn.Server(config)\n",
    "        await server.serve()\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Server error: {e}\")\n",
    "\n",
    "await start_frame_saver_server()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
